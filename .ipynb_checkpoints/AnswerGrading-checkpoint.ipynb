{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af280ddb-fe2c-4ca6-960f-e7df17ac5d81",
   "metadata": {},
   "source": [
    "# Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fd8987-6ef5-432e-ac2d-a6ce217feb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 22:46:07.618535: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739812567.634249   19312 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739812567.638988   19312 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-17 22:46:07.654431: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b1a8130-c468-4345-bdaa-6234736ac55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_points(text):\n",
    "#     \"\"\"Extract and number individual points from text.\"\"\"\n",
    "#     # Split text into sentences\n",
    "#     sentences = sent_tokenize(text)\n",
    "    \n",
    "#     # Clean and filter points\n",
    "#     points = []\n",
    "#     for sentence in sentences:\n",
    "#         # Clean the sentence\n",
    "#         cleaned = sentence.strip()\n",
    "#         # Filter out very short sentences and headings\n",
    "#         if len(cleaned.split()) > 3:  # Minimum words threshold\n",
    "#             points.append(cleaned)\n",
    "    \n",
    "#     # Number the points\n",
    "#     numbered_points = [f\"Point {i+1}: {point}\" for i, point in enumerate(points)]\n",
    "    \n",
    "#     return points, numbered_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7c2d589-5409-485b-a1b8-55efca0fc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matching_points(model_points, student_points):\n",
    "#     \"\"\"Find and visualize matching points.\"\"\"\n",
    "#     # Initialize the model\n",
    "#     model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    \n",
    "#     if not model_points or not student_points:\n",
    "#         return []\n",
    "    \n",
    "#     # Calculate embeddings\n",
    "#     model_embeddings = model.encode(model_points)\n",
    "#     student_embeddings = model.encode(student_points)\n",
    "    \n",
    "#     # Calculate similarity matrix\n",
    "#     similarity_matrix = util.pytorch_cos_sim(model_embeddings, student_embeddings)\n",
    "    \n",
    "#     matches = []\n",
    "#     used_student_points = set()\n",
    "    \n",
    "#     # Find best matches\n",
    "#     for i, model_point in enumerate(model_points):\n",
    "#         similarities = similarity_matrix[i]\n",
    "#         best_match_score = -1\n",
    "#         best_match_idx = -1\n",
    "        \n",
    "#         for j, score in enumerate(similarities):\n",
    "#             if j not in used_student_points and score > best_match_score:\n",
    "#                 best_match_score = score\n",
    "#                 best_match_idx = j\n",
    "        \n",
    "#         if best_match_idx != -1 and best_match_score > 0.5:  # Similarity threshold\n",
    "#             matches.append({\n",
    "#                 'model_point_idx': i,\n",
    "#                 'model_point': model_point,\n",
    "#                 'student_point_idx': best_match_idx,\n",
    "#                 'student_point': student_points[best_match_idx],\n",
    "#                 'similarity': float(best_match_score)\n",
    "#             })\n",
    "#             used_student_points.add(best_match_idx)\n",
    "    \n",
    "#     return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3014f16d-06ac-4875-a62b-42194de080be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grade_answer(model_answer, student_answer, max_marks):\n",
    "#     \"\"\"Grade and visualize point matching.\"\"\"\n",
    "#     # Extract points\n",
    "#     model_points, numbered_model_points = extract_points(model_answer)\n",
    "#     student_points, numbered_student_points = extract_points(student_answer)\n",
    "    \n",
    "#     # Find matching points\n",
    "#     matches = find_matching_points(model_points, student_points)\n",
    "    \n",
    "#     # Calculate marks\n",
    "#     total_points = len(model_points)\n",
    "#     marks_per_point = max_marks / total_points if total_points > 0 else 0\n",
    "#     total_marks = 0\n",
    "    \n",
    "#     # Prepare visualization data\n",
    "#     comparison_data = []\n",
    "#     used_student_indices = set()\n",
    "    \n",
    "#     for match in matches:\n",
    "#         point_marks = marks_per_point * match['similarity']\n",
    "#         total_marks += point_marks\n",
    "        \n",
    "#         comparison_data.append({\n",
    "#             'Model Point': numbered_model_points[match['model_point_idx']],\n",
    "#             'Student Point': numbered_student_points[match['student_point_idx']],\n",
    "#             'Similarity': f\"{match['similarity']:.2f}\",\n",
    "#             'Marks': f\"{point_marks:.2f}/{marks_per_point:.2f}\"\n",
    "#         })\n",
    "#         used_student_indices.add(match['student_point_idx'])\n",
    "    \n",
    "#     # Add unmatched points\n",
    "#     matched_model_indices = {match['model_point_idx'] for match in matches}\n",
    "#     for i in range(len(model_points)):\n",
    "#         if i not in matched_model_indices:\n",
    "#             comparison_data.append({\n",
    "#                 'Model Point': numbered_model_points[i],\n",
    "#                 'Student Point': \"‚ùå No matching point found\",\n",
    "#                 'Similarity': \"0.00\",\n",
    "#                 'Marks': f\"0.00/{marks_per_point:.2f}\"\n",
    "#             })\n",
    "    \n",
    "#     # Add extra student points that weren't matched\n",
    "#     for i in range(len(student_points)):\n",
    "#         if i not in used_student_indices:\n",
    "#             comparison_data.append({\n",
    "#                 'Model Point': \"‚ûï Extra student point\",\n",
    "#                 'Student Point': numbered_student_points[i],\n",
    "#                 'Similarity': \"N/A\",\n",
    "#                 'Marks': \"0.00\"\n",
    "#             })\n",
    "    \n",
    "#     return {\n",
    "#         'total_marks': round(total_marks, 2),\n",
    "#         'max_marks': max_marks,\n",
    "#         'points_matched': len(matches),\n",
    "#         'total_points': total_points,\n",
    "#         'comparison_data': comparison_data\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74e36d4f-7800-47a5-8510-28771dc3d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_grading_results(result):\n",
    "#     \"\"\"Display detailed grading results with color coding.\"\"\"\n",
    "#     print(f\"\\nüìù Grading Results:\")\n",
    "#     print(f\"Total Marks: {result['total_marks']}/{result['max_marks']}\")\n",
    "#     print(f\"Points Matched: {result['points_matched']}/{result['total_points']}\")\n",
    "    \n",
    "#     # Create DataFrame\n",
    "#     df = pd.DataFrame(result['comparison_data'])\n",
    "    \n",
    "#     # Style the DataFrame\n",
    "#     def color_similarity(val):\n",
    "#         try:\n",
    "#             similarity = float(val)\n",
    "#             if similarity >= 0.8:\n",
    "#                 return 'background-color: #90EE90'  # Light green\n",
    "#             elif similarity >= 0.5:\n",
    "#                 return 'background-color: #FFFFE0'  # Light yellow\n",
    "#             else:\n",
    "#                 return 'background-color: #FFB6C0'  # Light red\n",
    "#         except:\n",
    "#             return ''\n",
    "    \n",
    "#     # Apply styling\n",
    "#     styled_df = df.style.apply(lambda x: [''] * len(x) if x.name != 'Similarity' \n",
    "#                               else [color_similarity(v) for v in x], axis=0)\n",
    "    \n",
    "#     display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89ee00e3-fcb1-42ef-9dda-0bcde7ef65c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 2.74/10\n",
      "Points Matched: 2/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d1470_row0_col2, #T_d1470_row1_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "#T_d1470_row2_col2, #T_d1470_row3_col2, #T_d1470_row4_col2 {\n",
       "  background-color: #FFB6C0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d1470\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d1470_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_d1470_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_d1470_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_d1470_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d1470_row0_col0\" class=\"data row0 col0\" >Point 1: Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_d1470_row0_col1\" class=\"data row0 col1\" >Point 3: Cross-entropy works better than mean squared error (MSE) because MSE can only be used for binary classification.</td>\n",
       "      <td id=\"T_d1470_row0_col2\" class=\"data row0 col2\" >0.67</td>\n",
       "      <td id=\"T_d1470_row0_col3\" class=\"data row0 col3\" >1.34/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d1470_row1_col0\" class=\"data row1 col0\" >Point 4: It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_d1470_row1_col1\" class=\"data row1 col1\" >Point 4: This loss function penalizes incorrect predictions more heavily, making it more efficient.</td>\n",
       "      <td id=\"T_d1470_row1_col2\" class=\"data row1 col2\" >0.70</td>\n",
       "      <td id=\"T_d1470_row1_col3\" class=\"data row1 col3\" >1.40/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d1470_row2_col0\" class=\"data row2 col0\" >Point 2: It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_d1470_row2_col1\" class=\"data row2 col1\" >‚ùå No matching point found</td>\n",
       "      <td id=\"T_d1470_row2_col2\" class=\"data row2 col2\" >0.00</td>\n",
       "      <td id=\"T_d1470_row2_col3\" class=\"data row2 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d1470_row3_col0\" class=\"data row3 col0\" >Point 3: Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_d1470_row3_col1\" class=\"data row3 col1\" >‚ùå No matching point found</td>\n",
       "      <td id=\"T_d1470_row3_col2\" class=\"data row3 col2\" >0.00</td>\n",
       "      <td id=\"T_d1470_row3_col3\" class=\"data row3 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d1470_row4_col0\" class=\"data row4 col0\" >Point 5: By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_d1470_row4_col1\" class=\"data row4 col1\" >‚ùå No matching point found</td>\n",
       "      <td id=\"T_d1470_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_d1470_row4_col3\" class=\"data row4 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d1470_row5_col0\" class=\"data row5 col0\" >‚ûï Extra student point</td>\n",
       "      <td id=\"T_d1470_row5_col1\" class=\"data row5 col1\" >Point 1: Cross-entropy loss measures the distance between the true labels and predicted labels in terms of Euclidean distance.</td>\n",
       "      <td id=\"T_d1470_row5_col2\" class=\"data row5 col2\" >N/A</td>\n",
       "      <td id=\"T_d1470_row5_col3\" class=\"data row5 col3\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d1470_row6_col0\" class=\"data row6 col0\" >‚ûï Extra student point</td>\n",
       "      <td id=\"T_d1470_row6_col1\" class=\"data row6 col1\" >Point 2: It ensures that all classes are given equal probability, preventing overfitting.</td>\n",
       "      <td id=\"T_d1470_row6_col2\" class=\"data row6 col2\" >N/A</td>\n",
       "      <td id=\"T_d1470_row6_col3\" class=\"data row6 col3\" >0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d1470_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d1470_row7_col0\" class=\"data row7 col0\" >‚ûï Extra student point</td>\n",
       "      <td id=\"T_d1470_row7_col1\" class=\"data row7 col1\" >Point 5: It is based on the concept of information gain, which helps in minimizing the number of epochs required for training.</td>\n",
       "      <td id=\"T_d1470_row7_col2\" class=\"data row7 col2\" >N/A</td>\n",
       "      <td id=\"T_d1470_row7_col3\" class=\"data row7 col3\" >0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40b1a6cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - BAD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss measures the distance between the true labels and predicted labels in terms of Euclidean distance.\n",
    "2. It ensures that all classes are given equal probability, preventing overfitting.\n",
    "3. Cross-entropy works better than mean squared error (MSE) because MSE can only be used for binary classification.\n",
    "4. This loss function penalizes incorrect predictions more heavily, making it more efficient.\n",
    "5. It is based on the concept of information gain, which helps in minimizing the number of epochs required for training.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d73e1135-0113-459d-bb9b-2b3385808a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 6.17/10\n",
      "Points Matched: 4/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7c9fc_row0_col2, #T_7c9fc_row2_col2 {\n",
       "  background-color: #90EE90;\n",
       "}\n",
       "#T_7c9fc_row1_col2, #T_7c9fc_row3_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "#T_7c9fc_row4_col2 {\n",
       "  background-color: #FFB6C0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7c9fc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7c9fc_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_7c9fc_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_7c9fc_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_7c9fc_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7c9fc_row0_col0\" class=\"data row0 col0\" >Point 1: Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_7c9fc_row0_col1\" class=\"data row0 col1\" >Point 1: Cross-entropy loss provides a probabilistic interpretation, as it directly compares predicted probability distributions with true labels.</td>\n",
       "      <td id=\"T_7c9fc_row0_col2\" class=\"data row0 col2\" >0.81</td>\n",
       "      <td id=\"T_7c9fc_row0_col3\" class=\"data row0 col3\" >1.63/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7c9fc_row1_col0\" class=\"data row1 col0\" >Point 2: It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_7c9fc_row1_col1\" class=\"data row1 col1\" >Point 5: It ensures that softmax outputs are optimized, leading to better class separation.</td>\n",
       "      <td id=\"T_7c9fc_row1_col2\" class=\"data row1 col2\" >0.75</td>\n",
       "      <td id=\"T_7c9fc_row1_col3\" class=\"data row1 col3\" >1.50/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7c9fc_row2_col0\" class=\"data row2 col0\" >Point 3: Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_7c9fc_row2_col1\" class=\"data row2 col1\" >Point 2: Unlike mean squared error (MSE), which does not perform well for classification, cross-entropy does not suffer from vanishing gradients in deep networks.</td>\n",
       "      <td id=\"T_7c9fc_row2_col2\" class=\"data row2 col2\" >0.87</td>\n",
       "      <td id=\"T_7c9fc_row2_col3\" class=\"data row2 col3\" >1.74/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7c9fc_row3_col0\" class=\"data row3 col0\" >Point 4: It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_7c9fc_row3_col1\" class=\"data row3 col1\" >Point 3: It assigns higher penalties to incorrect predictions with high confidence, encouraging the model to be more confident in correct predictions.</td>\n",
       "      <td id=\"T_7c9fc_row3_col2\" class=\"data row3 col2\" >0.65</td>\n",
       "      <td id=\"T_7c9fc_row3_col3\" class=\"data row3 col3\" >1.30/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7c9fc_row4_col0\" class=\"data row4 col0\" >Point 5: By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_7c9fc_row4_col1\" class=\"data row4 col1\" >‚ùå No matching point found</td>\n",
       "      <td id=\"T_7c9fc_row4_col2\" class=\"data row4 col2\" >0.00</td>\n",
       "      <td id=\"T_7c9fc_row4_col3\" class=\"data row4 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7c9fc_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7c9fc_row5_col0\" class=\"data row5 col0\" >‚ûï Extra student point</td>\n",
       "      <td id=\"T_7c9fc_row5_col1\" class=\"data row5 col1\" >Point 4: The loss function is derived from information theory, making it mathematically well-suited for classification tasks.</td>\n",
       "      <td id=\"T_7c9fc_row5_col2\" class=\"data row5 col2\" >N/A</td>\n",
       "      <td id=\"T_7c9fc_row5_col3\" class=\"data row5 col3\" >0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40995aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - GOOD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss provides a probabilistic interpretation, as it directly compares predicted probability distributions with true labels.\n",
    "2. Unlike mean squared error (MSE), which does not perform well for classification, cross-entropy does not suffer from vanishing gradients in deep networks.\n",
    "3. It assigns higher penalties to incorrect predictions with high confidence, encouraging the model to be more confident in correct predictions.\n",
    "4. The loss function is derived from information theory, making it mathematically well-suited for classification tasks.\n",
    "5. It ensures that softmax outputs are optimized, leading to better class separation.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e22d6f4-abeb-4e8a-9e6f-407a4e7ebb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 8.14/10\n",
      "Points Matched: 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_81f66_row0_col2, #T_81f66_row2_col2, #T_81f66_row4_col2 {\n",
       "  background-color: #90EE90;\n",
       "}\n",
       "#T_81f66_row1_col2, #T_81f66_row3_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_81f66\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_81f66_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_81f66_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_81f66_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_81f66_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_81f66_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_81f66_row0_col0\" class=\"data row0 col0\" >Point 1: Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_81f66_row0_col1\" class=\"data row0 col1\" >Point 1: Cross-entropy loss is particularly effective for classification as it quantifies the dissimilarity between the predicted and actual probability distributions.</td>\n",
       "      <td id=\"T_81f66_row0_col2\" class=\"data row0 col2\" >0.88</td>\n",
       "      <td id=\"T_81f66_row0_col3\" class=\"data row0 col3\" >1.76/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f66_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_81f66_row1_col0\" class=\"data row1 col0\" >Point 2: It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_81f66_row1_col1\" class=\"data row1 col1\" >Point 2: It synergizes well with the softmax function, ensuring the output probabilities are properly normalized and enhancing class distinction.</td>\n",
       "      <td id=\"T_81f66_row1_col2\" class=\"data row1 col2\" >0.72</td>\n",
       "      <td id=\"T_81f66_row1_col3\" class=\"data row1 col3\" >1.43/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f66_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_81f66_row2_col0\" class=\"data row2 col0\" >Point 3: Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_81f66_row2_col1\" class=\"data row2 col1\" >Point 4: Unlike mean squared error (MSE), which can result in sluggish learning and weak gradient signals in classification tasks, cross-entropy maintains a strong gradient flow.</td>\n",
       "      <td id=\"T_81f66_row2_col2\" class=\"data row2 col2\" >0.90</td>\n",
       "      <td id=\"T_81f66_row2_col3\" class=\"data row2 col3\" >1.80/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f66_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_81f66_row3_col0\" class=\"data row3 col0\" >Point 4: It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_81f66_row3_col1\" class=\"data row3 col1\" >Point 3: This loss function discourages incorrect yet overconfident predictions by imposing a higher penalty on them.</td>\n",
       "      <td id=\"T_81f66_row3_col2\" class=\"data row3 col2\" >0.71</td>\n",
       "      <td id=\"T_81f66_row3_col3\" class=\"data row3 col3\" >1.42/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_81f66_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_81f66_row4_col0\" class=\"data row4 col0\" >Point 5: By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_81f66_row4_col1\" class=\"data row4 col1\" >Point 5: Minimizing cross-entropy essentially boosts the probability of the correct class, leading to more reliable and precise model predictions.</td>\n",
       "      <td id=\"T_81f66_row4_col2\" class=\"data row4 col2\" >0.87</td>\n",
       "      <td id=\"T_81f66_row4_col3\" class=\"data row4 col3\" >1.73/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40aaf00e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - PERFECT\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss is particularly effective for classification as it quantifies the dissimilarity between the predicted and actual probability distributions.\n",
    "2. It synergizes well with the softmax function, ensuring the output probabilities are properly normalized and enhancing class distinction.\n",
    "3. This loss function discourages incorrect yet overconfident predictions by imposing a higher penalty on them.\n",
    "4. Unlike mean squared error (MSE), which can result in sluggish learning and weak gradient signals in classification tasks, cross-entropy maintains a strong gradient flow.\n",
    "5. Minimizing cross-entropy essentially boosts the probability of the correct class, leading to more reliable and precise model predictions.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62e2c1-4ae5-44ac-a315-f31854ca65c8",
   "metadata": {},
   "source": [
    "# ROUND 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4275ec5-10d4-430e-9241-42e183ada784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dhruv/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8c29f8e-3b71-45c1-82dc-c5a1be632253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#     \"\"\"Advanced text cleaning.\"\"\"\n",
    "#     # Remove numbering and special characters\n",
    "#     text = re.sub(r'^\\d+[\\.)]\\s*', '', text)\n",
    "#     text = re.sub(r'[^\\w\\s.,]', '', text)\n",
    "#     # Standardize whitespace\n",
    "#     text = ' '.join(text.split())\n",
    "#     return text.strip()\n",
    "\n",
    "# def extract_key_phrases(text):\n",
    "#     \"\"\"Extract key phrases and technical terms.\"\"\"\n",
    "#     words = word_tokenize(text.lower())\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     # Keep technical terms and important words\n",
    "#     key_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "#     return set(key_words)\n",
    "\n",
    "# def extract_points(text):\n",
    "#     \"\"\"Extract points with enhanced splitting logic.\"\"\"\n",
    "#     # Split by multiple delimiters\n",
    "#     splits = re.split(r'(?<=[.!?])\\s+|\\n+|(?<=\\d\\.)\\s+|(?<=\\))\\s+', text)\n",
    "    \n",
    "#     points = []\n",
    "#     for split in splits:\n",
    "#         cleaned = clean_text(split)\n",
    "#         if len(cleaned.split()) >= 3:  # Minimum word threshold\n",
    "#             points.append({\n",
    "#                 'text': cleaned,\n",
    "#                 'key_phrases': extract_key_phrases(cleaned)\n",
    "#             })\n",
    "    \n",
    "#     return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd546957-4f06-498d-8203-ef3f8bf1306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_similarity_score(model_point, student_point, model):\n",
    "#     \"\"\"Calculate comprehensive similarity score using multiple metrics.\"\"\"\n",
    "#     # SBERT similarity\n",
    "#     embeddings1 = model.encode(model_point['text'], convert_to_tensor=True)\n",
    "#     embeddings2 = model.encode(student_point['text'], convert_to_tensor=True)\n",
    "#     semantic_sim = float(util.pytorch_cos_sim(embeddings1, embeddings2))\n",
    "    \n",
    "#     # Key phrase overlap\n",
    "#     common_phrases = model_point['key_phrases'].intersection(student_point['key_phrases'])\n",
    "#     total_phrases = model_point['key_phrases'].union(student_point['key_phrases'])\n",
    "#     phrase_sim = len(common_phrases) / len(total_phrases) if total_phrases else 0\n",
    "    \n",
    "#     # Weight the similarities (can be adjusted)\n",
    "#     final_sim = (0.7 * semantic_sim) + (0.3 * phrase_sim)\n",
    "#     return final_sim\n",
    "\n",
    "# def find_matching_points(model_points, student_points):\n",
    "#     \"\"\"Find matching points using enhanced similarity metrics.\"\"\"\n",
    "#     model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "#     matches = []\n",
    "#     used_student_points = set()\n",
    "    \n",
    "#     # Calculate full similarity matrix\n",
    "#     similarity_matrix = np.zeros((len(model_points), len(student_points)))\n",
    "#     for i, mp in enumerate(model_points):\n",
    "#         for j, sp in enumerate(student_points):\n",
    "#             similarity_matrix[i][j] = calculate_similarity_score(mp, sp, model)\n",
    "    \n",
    "#     # Find best matches using greedy approach\n",
    "#     for i, model_point in enumerate(model_points):\n",
    "#         best_match_score = -1\n",
    "#         best_match_idx = -1\n",
    "        \n",
    "#         for j, student_point in enumerate(student_points):\n",
    "#             if j not in used_student_points:\n",
    "#                 score = similarity_matrix[i][j]\n",
    "#                 if score > best_match_score:\n",
    "#                     best_match_score = score\n",
    "#                     best_match_idx = j\n",
    "        \n",
    "#         if best_match_idx != -1 and best_match_score > 0.5:  # Adjustable threshold\n",
    "#             matches.append({\n",
    "#                 'model_point': model_point['text'],\n",
    "#                 'student_point': student_points[best_match_idx]['text'],\n",
    "#                 'similarity': best_match_score\n",
    "#             })\n",
    "#             used_student_points.add(best_match_idx)\n",
    "    \n",
    "#     return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bad3b1b2-ab17-40b1-a584-0a2d6b6b1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grade_answer(model_answer, student_answer, max_marks):\n",
    "#     \"\"\"Grade answer with improved matching and scoring.\"\"\"\n",
    "#     # Extract points\n",
    "#     model_points = extract_points(model_answer)\n",
    "#     student_points = extract_points(student_answer)\n",
    "    \n",
    "#     # Find matches\n",
    "#     matches = find_matching_points(model_points, student_points)\n",
    "    \n",
    "#     # Calculate marks\n",
    "#     total_points = len(model_points)\n",
    "#     marks_per_point = max_marks / total_points if total_points > 0 else 0\n",
    "    \n",
    "#     # Prepare results\n",
    "#     comparison_data = []\n",
    "#     total_marks = 0\n",
    "    \n",
    "#     # Process matched points\n",
    "#     matched_model_points = set()\n",
    "#     for match in matches:\n",
    "#         point_marks = marks_per_point * match['similarity']\n",
    "#         total_marks += point_marks\n",
    "#         comparison_data.append({\n",
    "#             'Model Point': match['model_point'],\n",
    "#             'Student Point': match['student_point'],\n",
    "#             'Similarity': f\"{match['similarity']:.3f}\",\n",
    "#             'Marks': f\"{point_marks:.2f}/{marks_per_point:.2f}\"\n",
    "#         })\n",
    "#         matched_model_points.add(match['model_point'])\n",
    "    \n",
    "#     # Add unmatched model points\n",
    "#     for point in model_points:\n",
    "#         if point['text'] not in matched_model_points:\n",
    "#             comparison_data.append({\n",
    "#                 'Model Point': point['text'],\n",
    "#                 'Student Point': 'NO MATCH',\n",
    "#                 'Similarity': '0.000',\n",
    "#                 'Marks': f\"0.00/{marks_per_point:.2f}\"\n",
    "#             })\n",
    "    \n",
    "#     return {\n",
    "#         'total_marks': round(total_marks, 2),\n",
    "#         'max_marks': max_marks,\n",
    "#         'points_matched': len(matches),\n",
    "#         'total_points': total_points,\n",
    "#         'comparison_data': comparison_data\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51caa1f-739b-4634-a00c-4883af4369b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_results(result):\n",
    "#     \"\"\"Display results in a simple, clear format.\"\"\"\n",
    "#     print(f\"\\nGrading Summary:\")\n",
    "#     print(f\"Total Marks: {result['total_marks']}/{result['max_marks']}\")\n",
    "#     print(f\"Points Matched: {result['points_matched']}/{result['total_points']}\")\n",
    "    \n",
    "#     # Create and display DataFrame\n",
    "#     df = pd.DataFrame(result['comparison_data'])\n",
    "#     print(\"\\nDetailed Point Matching:\")\n",
    "#     display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c99e70-301f-4f9a-9ab8-3145271e432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 2.3/10\n",
      "Points Matched: 2/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f2fc1_row0_col2, #T_f2fc1_row1_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "#T_f2fc1_row2_col2, #T_f2fc1_row3_col2, #T_f2fc1_row4_col2 {\n",
       "  background-color: #FFB6C0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f2fc1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f2fc1_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_f2fc1_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_f2fc1_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_f2fc1_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fc1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f2fc1_row0_col0\" class=\"data row0 col0\" >Unlike mean squared error MSE, which can lead to slow convergence and vanishing gradients in classification, crossentropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_f2fc1_row0_col1\" class=\"data row0 col1\" >Crossentropy works better than mean squared error MSE</td>\n",
       "      <td id=\"T_f2fc1_row0_col2\" class=\"data row0 col2\" >0.577</td>\n",
       "      <td id=\"T_f2fc1_row0_col3\" class=\"data row0 col3\" >1.15/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fc1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f2fc1_row1_col0\" class=\"data row1 col0\" >It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_f2fc1_row1_col1\" class=\"data row1 col1\" >This loss function penalizes incorrect predictions more heavily, making it more efficient.</td>\n",
       "      <td id=\"T_f2fc1_row1_col2\" class=\"data row1 col2\" >0.575</td>\n",
       "      <td id=\"T_f2fc1_row1_col3\" class=\"data row1 col3\" >1.15/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fc1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f2fc1_row2_col0\" class=\"data row2 col0\" >Crossentropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_f2fc1_row2_col1\" class=\"data row2 col1\" >NO MATCH</td>\n",
       "      <td id=\"T_f2fc1_row2_col2\" class=\"data row2 col2\" >0.000</td>\n",
       "      <td id=\"T_f2fc1_row2_col3\" class=\"data row2 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fc1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f2fc1_row3_col0\" class=\"data row3 col0\" >It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_f2fc1_row3_col1\" class=\"data row3 col1\" >NO MATCH</td>\n",
       "      <td id=\"T_f2fc1_row3_col2\" class=\"data row3 col2\" >0.000</td>\n",
       "      <td id=\"T_f2fc1_row3_col3\" class=\"data row3 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f2fc1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f2fc1_row4_col0\" class=\"data row4 col0\" >By minimizing crossentropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_f2fc1_row4_col1\" class=\"data row4 col1\" >NO MATCH</td>\n",
       "      <td id=\"T_f2fc1_row4_col2\" class=\"data row4 col2\" >0.000</td>\n",
       "      <td id=\"T_f2fc1_row4_col3\" class=\"data row4 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40b1a6cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - BAD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss measures the distance between the true labels and predicted labels in terms of Euclidean distance.\n",
    "2. It ensures that all classes are given equal probability, preventing overfitting.\n",
    "3. Cross-entropy works better than mean squared error (MSE) because MSE can only be used for binary classification.\n",
    "4. This loss function penalizes incorrect predictions more heavily, making it more efficient.\n",
    "5. It is based on the concept of information gain, which helps in minimizing the number of epochs required for training.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4faf98d6-4cf1-461e-a5ab-a5641deebb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 4.82/10\n",
      "Points Matched: 4/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_25af5_row0_col2, #T_25af5_row1_col2, #T_25af5_row2_col2, #T_25af5_row3_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "#T_25af5_row4_col2 {\n",
       "  background-color: #FFB6C0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_25af5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_25af5_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_25af5_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_25af5_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_25af5_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_25af5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_25af5_row0_col0\" class=\"data row0 col0\" >Crossentropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_25af5_row0_col1\" class=\"data row0 col1\" >Crossentropy loss provides a probabilistic interpretation, as it directly compares predicted probability distributions with true labels.</td>\n",
       "      <td id=\"T_25af5_row0_col2\" class=\"data row0 col2\" >0.606</td>\n",
       "      <td id=\"T_25af5_row0_col3\" class=\"data row0 col3\" >1.21/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25af5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_25af5_row1_col0\" class=\"data row1 col0\" >It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_25af5_row1_col1\" class=\"data row1 col1\" >It ensures that softmax outputs are optimized, leading to better class separation.</td>\n",
       "      <td id=\"T_25af5_row1_col2\" class=\"data row1 col2\" >0.579</td>\n",
       "      <td id=\"T_25af5_row1_col3\" class=\"data row1 col3\" >1.16/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25af5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_25af5_row2_col0\" class=\"data row2 col0\" >Unlike mean squared error MSE, which can lead to slow convergence and vanishing gradients in classification, crossentropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_25af5_row2_col1\" class=\"data row2 col1\" >Unlike mean squared error MSE, which does not perform well for classification, crossentropy does not suffer from vanishing gradients in deep networks.</td>\n",
       "      <td id=\"T_25af5_row2_col2\" class=\"data row2 col2\" >0.720</td>\n",
       "      <td id=\"T_25af5_row2_col3\" class=\"data row2 col3\" >1.44/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25af5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_25af5_row3_col0\" class=\"data row3 col0\" >It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_25af5_row3_col1\" class=\"data row3 col1\" >It assigns higher penalties to incorrect predictions with high confidence, encouraging the model to be more confident in correct predictions.</td>\n",
       "      <td id=\"T_25af5_row3_col2\" class=\"data row3 col2\" >0.504</td>\n",
       "      <td id=\"T_25af5_row3_col3\" class=\"data row3 col3\" >1.01/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25af5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_25af5_row4_col0\" class=\"data row4 col0\" >By minimizing crossentropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_25af5_row4_col1\" class=\"data row4 col1\" >NO MATCH</td>\n",
       "      <td id=\"T_25af5_row4_col2\" class=\"data row4 col2\" >0.000</td>\n",
       "      <td id=\"T_25af5_row4_col3\" class=\"data row4 col3\" >0.00/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40b1a6cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - GOOD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss provides a probabilistic interpretation, as it directly compares predicted probability distributions with true labels.\n",
    "2. Unlike mean squared error (MSE), which does not perform well for classification, cross-entropy does not suffer from vanishing gradients in deep networks.\n",
    "3. It assigns higher penalties to incorrect predictions with high confidence, encouraging the model to be more confident in correct predictions.\n",
    "4. The loss function is derived from information theory, making it mathematically well-suited for classification tasks.\n",
    "5. It ensures that softmax outputs are optimized, leading to better class separation.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e67985d0-27f0-4207-9757-c23ef9b8f9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Grading Results:\n",
      "Total Marks: 6.55/10\n",
      "Points Matched: 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_70c7a_row0_col2, #T_70c7a_row1_col2, #T_70c7a_row2_col2, #T_70c7a_row3_col2, #T_70c7a_row4_col2 {\n",
       "  background-color: #FFFFE0;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_70c7a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_70c7a_level0_col0\" class=\"col_heading level0 col0\" >Model Point</th>\n",
       "      <th id=\"T_70c7a_level0_col1\" class=\"col_heading level0 col1\" >Student Point</th>\n",
       "      <th id=\"T_70c7a_level0_col2\" class=\"col_heading level0 col2\" >Similarity</th>\n",
       "      <th id=\"T_70c7a_level0_col3\" class=\"col_heading level0 col3\" >Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_70c7a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_70c7a_row0_col0\" class=\"data row0 col0\" >Crossentropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.</td>\n",
       "      <td id=\"T_70c7a_row0_col1\" class=\"data row0 col1\" >Crossentropy loss is particularly effective for classification as it quantifies the dissimilarity between the predicted and actual probability distributions.</td>\n",
       "      <td id=\"T_70c7a_row0_col2\" class=\"data row0 col2\" >0.684</td>\n",
       "      <td id=\"T_70c7a_row0_col3\" class=\"data row0 col3\" >1.37/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c7a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_70c7a_row1_col0\" class=\"data row1 col0\" >It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.</td>\n",
       "      <td id=\"T_70c7a_row1_col1\" class=\"data row1 col1\" >It synergizes well with the softmax function, ensuring the output probabilities are properly normalized and enhancing class distinction.</td>\n",
       "      <td id=\"T_70c7a_row1_col2\" class=\"data row1 col2\" >0.580</td>\n",
       "      <td id=\"T_70c7a_row1_col3\" class=\"data row1 col3\" >1.16/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c7a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_70c7a_row2_col0\" class=\"data row2 col0\" >Unlike mean squared error MSE, which can lead to slow convergence and vanishing gradients in classification, crossentropy maintains strong gradient signals.</td>\n",
       "      <td id=\"T_70c7a_row2_col1\" class=\"data row2 col1\" >Unlike mean squared error MSE, which can result in sluggish learning and weak gradient signals in classification tasks, crossentropy maintains a strong gradient flow.</td>\n",
       "      <td id=\"T_70c7a_row2_col2\" class=\"data row2 col2\" >0.767</td>\n",
       "      <td id=\"T_70c7a_row2_col3\" class=\"data row2 col3\" >1.53/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c7a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_70c7a_row3_col0\" class=\"data row3 col0\" >It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.</td>\n",
       "      <td id=\"T_70c7a_row3_col1\" class=\"data row3 col1\" >This loss function discourages incorrect yet overconfident predictions by imposing a higher penalty on them.</td>\n",
       "      <td id=\"T_70c7a_row3_col2\" class=\"data row3 col2\" >0.571</td>\n",
       "      <td id=\"T_70c7a_row3_col3\" class=\"data row3 col3\" >1.14/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_70c7a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_70c7a_row4_col0\" class=\"data row4 col0\" >By minimizing crossentropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.</td>\n",
       "      <td id=\"T_70c7a_row4_col1\" class=\"data row4 col1\" >Minimizing crossentropy essentially boosts the probability of the correct class, leading to more reliable and precise model predictions.</td>\n",
       "      <td id=\"T_70c7a_row4_col2\" class=\"data row4 col2\" >0.671</td>\n",
       "      <td id=\"T_70c7a_row4_col3\" class=\"data row4 col3\" >1.34/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x75f40b1a6cf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - PERFECT\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss is particularly effective for classification as it quantifies the dissimilarity between the predicted and actual probability distributions.\n",
    "2. It synergizes well with the softmax function, ensuring the output probabilities are properly normalized and enhancing class distinction.\n",
    "3. This loss function discourages incorrect yet overconfident predictions by imposing a higher penalty on them.\n",
    "4. Unlike mean squared error (MSE), which can result in sluggish learning and weak gradient signals in classification tasks, cross-entropy maintains a strong gradient flow.\n",
    "5. Minimizing cross-entropy essentially boosts the probability of the correct class, leading to more reliable and precise model predictions.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "# Grade and display results\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10)\n",
    "display_grading_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff091c6-b6f4-45a2-ad5c-499d1983536f",
   "metadata": {},
   "source": [
    "# Round 3 -Enchanced Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b108eba-8d9d-42ea-8e89-48b3a8e2de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Advanced text cleaning.\"\"\"\n",
    "    # Remove numbering and special characters\n",
    "    text = re.sub(r'^\\d+[\\.)]\\s*', '', text)\n",
    "    text = re.sub(r'[^\\w\\s.,]', '', text)\n",
    "    # Standardize whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def extract_key_phrases(text):\n",
    "    \"\"\"Extract key phrases and technical terms.\"\"\"\n",
    "    words = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Keep technical terms and important words\n",
    "    key_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return set(key_words)\n",
    "\n",
    "def extract_points(text):\n",
    "    \"\"\"Extract points with enhanced splitting logic.\"\"\"\n",
    "    # Split by multiple delimiters\n",
    "    splits = re.split(r'(?<=[.!?])\\s+|\\n+|(?<=\\d\\.)\\s+|(?<=\\))\\s+', text)\n",
    "    \n",
    "    points = []\n",
    "    for split in splits:\n",
    "        cleaned = clean_text(split)\n",
    "        if len(cleaned.split()) >= 3:  # Minimum word threshold\n",
    "            points.append({\n",
    "                'text': cleaned,\n",
    "                'key_phrases': extract_key_phrases(cleaned)\n",
    "            })\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5be9303e-7bda-43cc-9490-993042e5fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_score(model_point, student_point, model):\n",
    "    \"\"\"Calculate comprehensive similarity score using multiple metrics.\"\"\"\n",
    "    # SBERT similarity\n",
    "    embeddings1 = model.encode(model_point['text'], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(student_point['text'], convert_to_tensor=True)\n",
    "    semantic_sim = float(util.pytorch_cos_sim(embeddings1, embeddings2))\n",
    "    \n",
    "    # Key phrase overlap\n",
    "    common_phrases = model_point['key_phrases'].intersection(student_point['key_phrases'])\n",
    "    total_phrases = model_point['key_phrases'].union(student_point['key_phrases'])\n",
    "    phrase_sim = len(common_phrases) / len(total_phrases) if total_phrases else 0\n",
    "    \n",
    "    # Weight the similarities (can be adjusted)\n",
    "    final_sim = (0.7 * semantic_sim) + (0.3 * phrase_sim)\n",
    "    return final_sim\n",
    "\n",
    "def find_matching_points(model_points, student_points):\n",
    "    \"\"\"Find matching points using enhanced similarity metrics.\"\"\"\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    matches = []\n",
    "    used_student_points = set()\n",
    "    \n",
    "    # Calculate full similarity matrix\n",
    "    similarity_matrix = np.zeros((len(model_points), len(student_points)))\n",
    "    for i, mp in enumerate(model_points):\n",
    "        for j, sp in enumerate(student_points):\n",
    "            similarity_matrix[i][j] = calculate_similarity_score(mp, sp, model)\n",
    "    \n",
    "    # Find best matches using greedy approach\n",
    "    for i, model_point in enumerate(model_points):\n",
    "        best_match_score = -1\n",
    "        best_match_idx = -1\n",
    "        \n",
    "        for j, student_point in enumerate(student_points):\n",
    "            if j not in used_student_points:\n",
    "                score = similarity_matrix[i][j]\n",
    "                if score > best_match_score:\n",
    "                    best_match_score = score\n",
    "                    best_match_idx = j\n",
    "        \n",
    "        if best_match_idx != -1 and best_match_score > 0.5:  # Adjustable threshold\n",
    "            matches.append({\n",
    "                'model_point': model_point['text'],\n",
    "                'student_point': student_points[best_match_idx]['text'],\n",
    "                'similarity': best_match_score\n",
    "            })\n",
    "            used_student_points.add(best_match_idx)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3af196ca-ab5e-459c-b515-7d2e6a63eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_score(model_point, student_point, model):\n",
    "    \"\"\"Calculate comprehensive similarity score using multiple metrics.\"\"\"\n",
    "    # SBERT similarity\n",
    "    embeddings1 = model.encode(model_point['text'], convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(student_point['text'], convert_to_tensor=True)\n",
    "    semantic_sim = float(util.pytorch_cos_sim(embeddings1, embeddings2))\n",
    "    \n",
    "    # Key phrase overlap\n",
    "    common_phrases = model_point['key_phrases'].intersection(student_point['key_phrases'])\n",
    "    total_phrases = model_point['key_phrases'].union(student_point['key_phrases'])\n",
    "    phrase_sim = len(common_phrases) / len(total_phrases) if total_phrases else 0\n",
    "    \n",
    "    # Calculate length ratio (penalize if answer is too short or too long)\n",
    "    len_ratio = min(len(student_point['text']) / len(model_point['text']), \n",
    "                   len(model_point['text']) / len(student_point['text']))\n",
    "    \n",
    "    # Weighted combination\n",
    "    weights = {\n",
    "        'semantic': 0.6,\n",
    "        'phrase': 0.3,\n",
    "        'length': 0.1\n",
    "    }\n",
    "    \n",
    "    final_sim = (\n",
    "        weights['semantic'] * semantic_sim + \n",
    "        weights['phrase'] * phrase_sim + \n",
    "        weights['length'] * len_ratio\n",
    "    )\n",
    "    \n",
    "    return final_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9c5c10b-7b71-478e-8c2b-347363e9dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_answer(model_answer, student_answer, max_marks, grading_params=None):\n",
    "    \"\"\"Grade answer with comprehensive evaluation criteria.\"\"\"\n",
    "    if grading_params is None:\n",
    "        grading_params = {\n",
    "            'content_weight': 0.7,      # Weight for content matching\n",
    "            'structure_weight': 0.15,    # Weight for answer structure\n",
    "            'completion_weight': 0.15,   # Weight for completion\n",
    "            'min_similarity': 0.5,       # Minimum similarity threshold\n",
    "            'length_penalty': 0.1,       # Penalty for length mismatch\n",
    "            'key_terms_weight': 0.2      # Weight for key terms within content\n",
    "        }\n",
    "    \n",
    "    # Extract points\n",
    "    model_points = extract_points(model_answer)\n",
    "    student_points = extract_points(student_answer)\n",
    "    \n",
    "    # Find matches\n",
    "    matches = find_matching_points(model_points, student_points)\n",
    "    \n",
    "    # Calculate base marks from content matching\n",
    "    total_points = len(model_points)\n",
    "    base_marks_per_point = max_marks / total_points if total_points > 0 else 0\n",
    "    \n",
    "    # Initialize grading components\n",
    "    content_score = 0\n",
    "    matched_count = 0\n",
    "    \n",
    "    # Process matched points\n",
    "    comparison_data = []\n",
    "    for match in matches:\n",
    "        if match['similarity'] >= grading_params['min_similarity']:\n",
    "            point_marks = base_marks_per_point * match['similarity']\n",
    "            content_score += point_marks\n",
    "            matched_count += 1\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model Point': match['model_point'],\n",
    "                'Student Point': match['student_point'],\n",
    "                'Similarity': f\"{match['similarity']:.3f}\",\n",
    "                'Point Marks': f\"{point_marks:.2f}/{base_marks_per_point:.2f}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate structure score\n",
    "    structure_score = matched_count / total_points if total_points > 0 else 0\n",
    "    \n",
    "    # Calculate completion score\n",
    "    completion_ratio = matched_count / total_points\n",
    "    completion_score = min(1.0, completion_ratio)\n",
    "    \n",
    "    # Calculate length penalty\n",
    "    expected_length = sum(len(p['text'].split()) for p in model_points)\n",
    "    actual_length = sum(len(p['text'].split()) for p in student_points)\n",
    "    length_ratio = min(actual_length / expected_length, expected_length / actual_length)\n",
    "    length_penalty = (1 - length_ratio) * grading_params['length_penalty']\n",
    "    \n",
    "    # Calculate final marks\n",
    "    final_marks = (\n",
    "        (content_score * grading_params['content_weight']) +\n",
    "        (structure_score * max_marks * grading_params['structure_weight']) +\n",
    "        (completion_score * max_marks * grading_params['completion_weight'])\n",
    "    )\n",
    "    \n",
    "    # Apply length penalty\n",
    "    final_marks = final_marks * (1 - length_penalty)\n",
    "    \n",
    "    # Add unmatched model points to comparison data\n",
    "    matched_model_points = set(match['model_point'] for match in matches)\n",
    "    for point in model_points:\n",
    "        if point['text'] not in matched_model_points:\n",
    "            comparison_data.append({\n",
    "                'Model Point': point['text'],\n",
    "                'Student Point': 'NO MATCH',\n",
    "                'Similarity': '0.000',\n",
    "                'Point Marks': f\"0.00/{base_marks_per_point:.2f}\"\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'total_marks': round(final_marks, 2),\n",
    "        'max_marks': max_marks,\n",
    "        'points_matched': matched_count,\n",
    "        'total_points': total_points,\n",
    "        'comparison_data': comparison_data,\n",
    "        'grading_components': {\n",
    "            'content_score': round(content_score, 2),\n",
    "            'structure_score': round(structure_score, 2),\n",
    "            'completion_score': round(completion_score, 2),\n",
    "            'length_penalty': round(length_penalty, 2)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f95dc47-df33-45de-9215-707cf5363ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_detailed_results(result):\n",
    "    \"\"\"Display comprehensive grading results.\"\"\"\n",
    "    print(\"\\nGrading Summary:\")\n",
    "    print(f\"Total Marks: {result['total_marks']}/{result['max_marks']}\")\n",
    "    print(f\"Points Matched: {result['points_matched']}/{result['total_points']}\")\n",
    "    \n",
    "    print(\"\\nGrading Components:\")\n",
    "    components = result['grading_components']\n",
    "    print(f\"Content Score: {components['content_score']}\")\n",
    "    print(f\"Structure Score: {components['structure_score']:.2f}\")\n",
    "    print(f\"Completion Score: {components['completion_score']:.2f}\")\n",
    "    print(f\"Length Penalty: {components['length_penalty']:.2f}\")\n",
    "    \n",
    "    print(\"\\nDetailed Point Matching:\")\n",
    "    df = pd.DataFrame(result['comparison_data'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11f07845-5e57-4009-aec1-cf03896232ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    'content_weight': 0.7,\n",
    "    'structure_weight': 0.15,\n",
    "    'completion_weight': 0.15,\n",
    "    'min_similarity': 0.5,\n",
    "    'length_penalty': 0.1,\n",
    "    'key_terms_weight': 0.2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b63688d-2f9e-435e-a823-ab980fbc7db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grading Summary:\n",
      "Total Marks: 2.72/10\n",
      "Points Matched: 2/5\n",
      "\n",
      "Grading Components:\n",
      "Content Score: 2.23\n",
      "Structure Score: 0.40\n",
      "Completion Score: 0.40\n",
      "Length Penalty: 0.02\n",
      "\n",
      "Detailed Point Matching:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Point</th>\n",
       "      <th>Student Point</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Point Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unlike mean squared error MSE, which can lead ...</td>\n",
       "      <td>Crossentropy works better than mean squared er...</td>\n",
       "      <td>0.540</td>\n",
       "      <td>1.08/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It penalizes incorrect classifications more ag...</td>\n",
       "      <td>This loss function penalizes incorrect predict...</td>\n",
       "      <td>0.574</td>\n",
       "      <td>1.15/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crossentropy loss is derived from information ...</td>\n",
       "      <td>NO MATCH</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It works well with softmax activation, ensurin...</td>\n",
       "      <td>NO MATCH</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By minimizing crossentropy, the model maximize...</td>\n",
       "      <td>NO MATCH</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model Point  \\\n",
       "0  Unlike mean squared error MSE, which can lead ...   \n",
       "1  It penalizes incorrect classifications more ag...   \n",
       "2  Crossentropy loss is derived from information ...   \n",
       "3  It works well with softmax activation, ensurin...   \n",
       "4  By minimizing crossentropy, the model maximize...   \n",
       "\n",
       "                                       Student Point Similarity Point Marks  \n",
       "0  Crossentropy works better than mean squared er...      0.540   1.08/2.00  \n",
       "1  This loss function penalizes incorrect predict...      0.574   1.15/2.00  \n",
       "2                                           NO MATCH      0.000   0.00/2.00  \n",
       "3                                           NO MATCH      0.000   0.00/2.00  \n",
       "4                                           NO MATCH      0.000   0.00/2.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - BAD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss measures the distance between the true labels and predicted labels in terms of Euclidean distance.\n",
    "2. It ensures that all classes are given equal probability, preventing overfitting.\n",
    "3. Cross-entropy works better than mean squared error (MSE) because MSE can only be used for binary classification.\n",
    "4. This loss function penalizes incorrect predictions more heavily, making it more efficient.\n",
    "5. It is based on the concept of information gain, which helps in minimizing the number of epochs required for training.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10, grading_params=custom_params)\n",
    "display_detailed_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "54e5ee84-5494-443f-a864-a679a2984d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grading Summary:\n",
      "Total Marks: 5.79/10\n",
      "Points Matched: 4/5\n",
      "\n",
      "Grading Components:\n",
      "Content Score: 4.9\n",
      "Structure Score: 0.80\n",
      "Completion Score: 0.80\n",
      "Length Penalty: 0.01\n",
      "\n",
      "Detailed Point Matching:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Point</th>\n",
       "      <th>Student Point</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Point Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crossentropy loss is derived from information ...</td>\n",
       "      <td>Crossentropy loss provides a probabilistic int...</td>\n",
       "      <td>0.615</td>\n",
       "      <td>1.23/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It works well with softmax activation, ensurin...</td>\n",
       "      <td>It ensures that softmax outputs are optimized,...</td>\n",
       "      <td>0.570</td>\n",
       "      <td>1.14/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike mean squared error MSE, which can lead ...</td>\n",
       "      <td>Unlike mean squared error MSE, which does not ...</td>\n",
       "      <td>0.732</td>\n",
       "      <td>1.46/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It penalizes incorrect classifications more ag...</td>\n",
       "      <td>It assigns higher penalties to incorrect predi...</td>\n",
       "      <td>0.532</td>\n",
       "      <td>1.06/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By minimizing crossentropy, the model maximize...</td>\n",
       "      <td>NO MATCH</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model Point  \\\n",
       "0  Crossentropy loss is derived from information ...   \n",
       "1  It works well with softmax activation, ensurin...   \n",
       "2  Unlike mean squared error MSE, which can lead ...   \n",
       "3  It penalizes incorrect classifications more ag...   \n",
       "4  By minimizing crossentropy, the model maximize...   \n",
       "\n",
       "                                       Student Point Similarity Point Marks  \n",
       "0  Crossentropy loss provides a probabilistic int...      0.615   1.23/2.00  \n",
       "1  It ensures that softmax outputs are optimized,...      0.570   1.14/2.00  \n",
       "2  Unlike mean squared error MSE, which does not ...      0.732   1.46/2.00  \n",
       "3  It assigns higher penalties to incorrect predi...      0.532   1.06/2.00  \n",
       "4                                           NO MATCH      0.000   0.00/2.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - GOOD\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss provides a probabilistic interpretation, as it directly compares predicted probability distributions with true labels.\n",
    "2. Unlike mean squared error (MSE), which does not perform well for classification, cross-entropy does not suffer from vanishing gradients in deep networks.\n",
    "3. It assigns higher penalties to incorrect predictions with high confidence, encouraging the model to be more confident in correct predictions.\n",
    "4. The loss function is derived from information theory, making it mathematically well-suited for classification tasks.\n",
    "5. It ensures that softmax outputs are optimized, leading to better class separation.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10, grading_params=custom_params)\n",
    "display_detailed_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "52ae5c01-0590-4a78-b0cd-445bb950c1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grading Summary:\n",
      "Total Marks: 7.64/10\n",
      "Points Matched: 5/5\n",
      "\n",
      "Grading Components:\n",
      "Content Score: 6.66\n",
      "Structure Score: 1.00\n",
      "Completion Score: 1.00\n",
      "Length Penalty: 0.00\n",
      "\n",
      "Detailed Point Matching:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Point</th>\n",
       "      <th>Student Point</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Point Marks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crossentropy loss is derived from information ...</td>\n",
       "      <td>Crossentropy loss is particularly effective fo...</td>\n",
       "      <td>0.697</td>\n",
       "      <td>1.39/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It works well with softmax activation, ensurin...</td>\n",
       "      <td>It synergizes well with the softmax function, ...</td>\n",
       "      <td>0.599</td>\n",
       "      <td>1.20/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unlike mean squared error MSE, which can lead ...</td>\n",
       "      <td>Unlike mean squared error MSE, which can resul...</td>\n",
       "      <td>0.773</td>\n",
       "      <td>1.55/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It penalizes incorrect classifications more ag...</td>\n",
       "      <td>This loss function discourages incorrect yet o...</td>\n",
       "      <td>0.582</td>\n",
       "      <td>1.16/2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By minimizing crossentropy, the model maximize...</td>\n",
       "      <td>Minimizing crossentropy essentially boosts the...</td>\n",
       "      <td>0.680</td>\n",
       "      <td>1.36/2.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Model Point  \\\n",
       "0  Crossentropy loss is derived from information ...   \n",
       "1  It works well with softmax activation, ensurin...   \n",
       "2  Unlike mean squared error MSE, which can lead ...   \n",
       "3  It penalizes incorrect classifications more ag...   \n",
       "4  By minimizing crossentropy, the model maximize...   \n",
       "\n",
       "                                       Student Point Similarity Point Marks  \n",
       "0  Crossentropy loss is particularly effective fo...      0.697   1.39/2.00  \n",
       "1  It synergizes well with the softmax function, ...      0.599   1.20/2.00  \n",
       "2  Unlike mean squared error MSE, which can resul...      0.773   1.55/2.00  \n",
       "3  This loss function discourages incorrect yet o...      0.582   1.16/2.00  \n",
       "4  Minimizing crossentropy essentially boosts the...      0.680   1.36/2.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example answers  - PERFECT\n",
    "student_answer1 = \"\"\"\n",
    "1. Cross-entropy loss is particularly effective for classification as it quantifies the dissimilarity between the predicted and actual probability distributions.\n",
    "2. It synergizes well with the softmax function, ensuring the output probabilities are properly normalized and enhancing class distinction.\n",
    "3. This loss function discourages incorrect yet overconfident predictions by imposing a higher penalty on them.\n",
    "4. Unlike mean squared error (MSE), which can result in sluggish learning and weak gradient signals in classification tasks, cross-entropy maintains a strong gradient flow.\n",
    "5. Minimizing cross-entropy essentially boosts the probability of the correct class, leading to more reliable and precise model predictions.\n",
    "\"\"\"\n",
    "\n",
    "model_answer = \"\"\"\n",
    "1. Cross-entropy loss is derived from information theory and measures the difference between two probability distributions, making it ideal for classification.\n",
    "2. It works well with softmax activation, ensuring that the predicted probabilities sum to one and optimizing class separation.\n",
    "3. Unlike mean squared error (MSE), which can lead to slow convergence and vanishing gradients in classification, cross-entropy maintains strong gradient signals.\n",
    "4. It penalizes incorrect classifications more aggressively, preventing the model from making overconfident yet incorrect predictions.\n",
    "5. By minimizing cross-entropy, the model maximizes the likelihood of the correct class, improving overall accuracy and robustness.\n",
    "\"\"\"\n",
    "\n",
    "result = grade_answer(model_answer, student_answer1, max_marks=10, grading_params=custom_params)\n",
    "display_detailed_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88692f-6932-46e9-a4bd-b6353b2dae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
