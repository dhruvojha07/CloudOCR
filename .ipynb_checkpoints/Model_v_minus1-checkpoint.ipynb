{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f11483-d086-4a21-8d8b-ed6f808e3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show keras tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecf9bc6-431e-4ca7-8cd8-b68f7f0eb6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 23:28:04.933410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738000684.947063    9009 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738000684.951229    9009 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-27 23:28:04.964294: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "from pdf2image import convert_from_path\n",
    "from google.cloud import vision\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0623afa4-ea84-447a-b8f4-590e13793be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    \"\"\"Convert a PDF to images, one image per page.\"\"\"\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = f\"{output_folder}/page_{i + 1}.jpg\"\n",
    "        image.save(image_path, \"JPEG\")\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763d2831-32dc-45e1-8913-f2d080aacb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images(image_paths):\n",
    "    \"\"\"Extract text from a list of image paths using Google Cloud Vision.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    all_text = \"\"\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = client.document_text_detection(image=image)\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f\"Error processing {image_path}: {response.error.message}\")\n",
    "\n",
    "        all_text += response.full_text_annotation.text + \"\\n\"\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29b4120-2e91-489f-9c00-2577540d1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_without_buckets(pdf_path, output_folder):\n",
    "    \"\"\"Process a PDF file without using Google Cloud Storage.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(pdf_path, output_folder)\n",
    "\n",
    "    print(\"Extracting text from images...\")\n",
    "    extracted_text = extract_text_from_images(image_paths)\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e4ebbef-dc08-48dc-95c4-c633446a03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by removing stopwords, punctuation, and lowercasing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6713b5a4-6c2b-428a-98af-526d3442386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_score(text1, text2):\n",
    "    \"\"\"Calculate the similarity score between two texts using SBERT.\"\"\"\n",
    "    sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    embeddings1 = sbert_model.encode(text1, convert_to_tensor=True)\n",
    "    embeddings2 = sbert_model.encode(text2, convert_to_tensor=True)\n",
    "    cosine_similarity = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    return cosine_similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f0a40d-f80c-4fd3-ba62-38c1042f8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(student_pdf_path, teacher_answer_path, output_folder):\n",
    "    # Extract text from student's PDF\n",
    "    print(\"Processing student's PDF...\")\n",
    "    student_text = process_pdf_without_buckets(student_pdf_path, output_folder)\n",
    "    \n",
    "    # If teacher's answer is a PDF, extract text; otherwise, read as text\n",
    "    if teacher_answer_path.endswith('.pdf'):\n",
    "        print(\"Processing teacher's PDF...\")\n",
    "        teacher_text = process_pdf_without_buckets(teacher_answer_path, output_folder)\n",
    "    else:\n",
    "        with open(teacher_answer_path, 'r') as file:\n",
    "            teacher_text = file.read()\n",
    "    \n",
    "    # Preprocess both texts\n",
    "    print(\"Preprocessing texts...\")\n",
    "    student_text_processed = preprocess_text(student_text)\n",
    "    teacher_text_processed = preprocess_text(teacher_text)\n",
    "    \n",
    "    # Calculate similarity score\n",
    "    print(\"Calculating similarity score...\")\n",
    "    similarity_score = get_similarity_score(student_text_processed, teacher_text_processed)\n",
    "    \n",
    "    # Assign marks based on similarity score (example: out of 10)\n",
    "    marks = round(similarity_score * 10, 2)\n",
    "    \n",
    "    print(f\"Similarity Score: {similarity_score}\")\n",
    "    print(f\"Marks Awarded: {marks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8391aa1-2281-4a59-a9e5-c2961c98b889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
