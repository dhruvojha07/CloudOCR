{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4612e26-f250-44f2-ac3e-7e8bbdb52e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "[nltk_data] Downloading package punkt to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97170131-9c5c-4984-ab9b-68bee4fc3717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "    def process_content(self, text: str) -> Dict[str, Any]:\n",
    "        chunks = self.content_chunking(text)\n",
    "        cleaned_chunks = self.text_cleaning(chunks)\n",
    "        metadata = self.metadata_extraction(text)\n",
    "        \n",
    "        return {\n",
    "            'chunks': cleaned_chunks,\n",
    "            'metadata': metadata,\n",
    "            'embeddings': self.generate_embeddings(cleaned_chunks)\n",
    "        }\n",
    "    \n",
    "    def content_chunking(self, text: str, chunk_size: int = 512) -> List[str]:\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(word_tokenize(sentence))\n",
    "            if current_length + sentence_length > chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "                \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "    \n",
    "    def text_cleaning(self, chunks: List[str]) -> List[str]:\n",
    "        cleaned_chunks = []\n",
    "        for chunk in chunks:\n",
    "            cleaned = re.sub(r'[^\\w\\s]', '', chunk)\n",
    "            cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "            cleaned = cleaned.strip().lower()\n",
    "            cleaned_chunks.append(cleaned)\n",
    "        return cleaned_chunks\n",
    "    \n",
    "    def metadata_extraction(self, text: str) -> Dict[str, Any]:\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        metadata = {\n",
    "            'entities': [(ent.text, ent.label_) for ent in doc.ents],\n",
    "            'word_count': len(word_tokenize(text)),\n",
    "            'sentence_count': len(sent_tokenize(text)),\n",
    "            'language': doc.lang_\n",
    "        }\n",
    "        return metadata\n",
    "    \n",
    "    def generate_embeddings(self, chunks: List[str]) -> np.ndarray:\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for chunk in chunks:\n",
    "                inputs = self.tokenizer(chunk, return_tensors='pt', \n",
    "                                      padding=True, truncation=True)\n",
    "                outputs = self.model(**inputs)\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "        return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46beee6b-e3d4-4970-ab27-b6d503d3b2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
