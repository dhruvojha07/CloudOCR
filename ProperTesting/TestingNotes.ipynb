{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17795cec-d1b2-4a8c-a759-c52bdd3255ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 10:24:35.828443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743828876.188841    4986 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743828876.308745    4986 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 10:24:37.275049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pdf2image import convert_from_path\n",
    "from google.cloud import vision\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ff8a44-b587-4686-8e63-25554bf4bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path, output_folder):\n",
    "    \"\"\"Convert a PDF to images, one image per page.\"\"\"\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = f\"{output_folder}/page_{i + 1}.jpg\"\n",
    "        image.save(image_path, \"JPEG\")\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4025dff4-52a9-4f13-a3d5-3bd9400c0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images(image_paths):\n",
    "    \"\"\"Extract text from a list of image paths using Google Cloud Vision.\"\"\"\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    all_text = \"\"\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "        \n",
    "        image = vision.Image(content=content)\n",
    "        response = client.document_text_detection(image=image)\n",
    "\n",
    "        if response.error.message:\n",
    "            raise Exception(f\"Error processing {image_path}: {response.error.message}\")\n",
    "\n",
    "        all_text += response.full_text_annotation.text + \"\\n\"\n",
    "\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad7d460-29a0-4071-ad38-4030efed7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_without_buckets(pdf_path, output_folder):\n",
    "    \"\"\"Process a PDF file without using Google Cloud Storage.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(pdf_path, output_folder)\n",
    "\n",
    "    print(\"Extracting text from images...\")\n",
    "    extracted_text = extract_text_from_images(image_paths)\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32435981-4ebb-435d-a707-5329afb25588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n+', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', '', text)  \n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(filtered_text), ' '.join(filtered_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "553b4e69-f0c7-4ffb-a639-cd9b3f857b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "\n",
    "def evaluate_answer(student_text, reference_text, total_marks):\n",
    "    \"\"\"Comprehensive answer evaluation with scaling based on total marks.\"\"\"\n",
    "    sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    def get_semantic_similarity():\n",
    "        \"\"\"Calculate semantic similarity using SBERT.\"\"\"\n",
    "        embeddings1 = sbert_model.encode(student_text, convert_to_tensor=True)\n",
    "        embeddings2 = sbert_model.encode(reference_text, convert_to_tensor=True)\n",
    "        return util.pytorch_cos_sim(embeddings1, embeddings2).item()\n",
    "\n",
    "    def check_length_ratio():\n",
    "        \"\"\"Check if answer length is appropriate.\"\"\"\n",
    "        student_length = len(word_tokenize(student_text))\n",
    "        reference_length = len(word_tokenize(reference_text))\n",
    "        ratio = student_length / reference_length if reference_length > 0 else 0\n",
    "        return min(1.0, ratio if ratio <= 1.5 else 1.5 / ratio)\n",
    "\n",
    "    def check_key_phrases():\n",
    "        \"\"\"Check for presence of key phrases and concepts.\"\"\"\n",
    "        def get_phrases(text):\n",
    "            words = word_tokenize(text.lower())\n",
    "            return {f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)} | \\\n",
    "                   {f\"{words[i]} {words[i+1]} {words[i+2]}\" for i in range(len(words)-2)}\n",
    "\n",
    "        ref_phrases = get_phrases(reference_text)\n",
    "        student_phrases = get_phrases(student_text)\n",
    "        return len(student_phrases & ref_phrases) / len(ref_phrases) if ref_phrases else 0\n",
    "\n",
    "    def check_sequence_alignment():\n",
    "        \"\"\"Check if ideas are presented in a similar sequence.\"\"\"\n",
    "        student_sentences = sent_tokenize(student_text)\n",
    "        reference_sentences = sent_tokenize(reference_text)\n",
    "        \n",
    "        student_emb = sbert_model.encode(student_sentences)\n",
    "        reference_emb = sbert_model.encode(reference_sentences)\n",
    "        \n",
    "        alignment_scores = [\n",
    "            util.pytorch_cos_sim(student_emb[i], reference_emb[i]).item()\n",
    "            for i in range(min(len(student_emb), len(reference_emb)))\n",
    "        ]\n",
    "        \n",
    "        return sum(alignment_scores) / len(alignment_scores) if alignment_scores else 0\n",
    "\n",
    "    def check_factual_accuracy():\n",
    "        \"\"\"Check for presence of numerical values and specific facts.\"\"\"\n",
    "        extract_numbers = lambda text: set(re.findall(r'\\d+(?:\\.\\d+)?', text))\n",
    "        \n",
    "        student_numbers = extract_numbers(student_text)\n",
    "        reference_numbers = extract_numbers(reference_text)\n",
    "        \n",
    "        return len(student_numbers & reference_numbers) / len(reference_numbers) if reference_numbers else 1.0\n",
    "\n",
    "    def check_coherence():\n",
    "        \"\"\"Check text coherence using sentence transitions.\"\"\"\n",
    "        sentences = sent_tokenize(student_text)\n",
    "        if len(sentences) < 2:\n",
    "            return 1.0\n",
    "            \n",
    "        coherence_scores = [\n",
    "            util.pytorch_cos_sim(\n",
    "                sbert_model.encode(sentences[i]), \n",
    "                sbert_model.encode(sentences[i+1])\n",
    "            ).item()\n",
    "            for i in range(len(sentences)-1)\n",
    "        ]\n",
    "            \n",
    "        return sum(coherence_scores) / len(coherence_scores)\n",
    "\n",
    "    scores = {\n",
    "        'semantic_similarity': get_semantic_similarity(),\n",
    "        'length_appropriateness': check_length_ratio(),\n",
    "        'key_phrases': check_key_phrases(),\n",
    "        'sequence_alignment': check_sequence_alignment(),\n",
    "        'factual_accuracy': check_factual_accuracy(),\n",
    "        'coherence': check_coherence()\n",
    "    }\n",
    "\n",
    "    weights = {\n",
    "        'semantic_similarity': 0.30,\n",
    "        'length_appropriateness': 0.15,\n",
    "        'key_phrases': 0.20,\n",
    "        'sequence_alignment': 0.15,\n",
    "        'factual_accuracy': 0.10,\n",
    "        'coherence': 0.10\n",
    "    }\n",
    "\n",
    "    raw_score = sum(scores[metric] * weights[metric] for metric in scores)\n",
    "    scaled_score = raw_score * total_marks  # Scale the score based on total marks\n",
    "\n",
    "    return {\n",
    "        'final_score': scaled_score,\n",
    "        'details': scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b75700b-51ef-4f41-92ba-a863d2692a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main(student_pdf_path, teacher_answer_path, output_folder, total_marks=10):\n",
    "    \"\"\"Main function with comprehensive evaluation and scaled scoring.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"Processing student's submission...\")\n",
    "        student_text = process_pdf_without_buckets(student_pdf_path, output_folder)\n",
    "        \n",
    "        print(\"Processing teacher's answer...\")\n",
    "        if teacher_answer_path.endswith('.pdf'):\n",
    "            teacher_text = process_pdf_without_buckets(teacher_answer_path, output_folder)\n",
    "        else:\n",
    "            with open(teacher_answer_path, 'r') as file:\n",
    "                teacher_text = file.read()\n",
    "        \n",
    "        print(\"Preprocessing texts...\")\n",
    "        student_processed, student_lemmatized = preprocess_text(student_text)\n",
    "        teacher_processed, teacher_lemmatized = preprocess_text(teacher_text)\n",
    "        \n",
    "        print(\"Evaluating answer...\")\n",
    "        evaluation_result = evaluate_answer(student_processed, teacher_processed, total_marks)\n",
    "        \n",
    "        marks = round(evaluation_result['final_score'], 2)  # Already scaled in evaluate_answer\n",
    "        \n",
    "        print(\"\\n=== Grading Report ===\")\n",
    "        print(f\"Final Marks: {marks}/{total_marks}\")\n",
    "        print(\"\\nDetailed Scores:\")\n",
    "        for metric, score in evaluation_result['details'].items():\n",
    "            print(f\"{metric.replace('_', ' ').title()}: {score:.2f}\")\n",
    "        \n",
    "        print(f\"\\nProcessing Time: {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return evaluation_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main processing: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506b592-c749-42ff-9caa-03a16e383c40",
   "metadata": {},
   "source": [
    "# Jeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2b96cd-6c49-4cde-8c3e-622dcc80b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.06/3\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.55\n",
      "Length Appropriateness: 0.30\n",
      "Key Phrases: 0.01\n",
      "Sequence Alignment: 0.28\n",
      "Factual Accuracy: 0.60\n",
      "Coherence: 0.40\n",
      "\n",
      "Processing Time: 8.45 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Jeet/Jeet-D038-Q1.pdf\"\n",
    "teacher_answer = \"Q1Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbcb193-7285-46c2-8b87-404e5f2f865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 3.15/7\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.54\n",
      "Length Appropriateness: 1.00\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.24\n",
      "Factual Accuracy: 0.67\n",
      "Coherence: 0.37\n",
      "\n",
      "Processing Time: 7.36 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Jeet/Jeet-D038-Q2.pdf\"\n",
    "teacher_answer = \"Q2-i-answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb58e379-3c07-46cc-b6f3-d1a03245ce6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.02/5\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.31\n",
      "Length Appropriateness: 0.37\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.13\n",
      "Factual Accuracy: 0.27\n",
      "Coherence: 0.10\n",
      "\n",
      "Processing Time: 5.59 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Jeet/Jeet-D038-Q3.pdf\"\n",
    "teacher_answer = \"Q3Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79eeb76-c4f6-44c5-81cc-83f4e5e22ee8",
   "metadata": {},
   "source": [
    "# Joel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6531e5c9-b1ed-42e7-bd33-39ddeda11c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 0.86/3\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.59\n",
      "Length Appropriateness: 0.13\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.18\n",
      "Factual Accuracy: 0.40\n",
      "Coherence: 0.23\n",
      "\n",
      "Processing Time: 7.03 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Joel/Joel-D041-Q1.pdf\"\n",
    "teacher_answer = \"Q1Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc2f415d-813c-407d-b842-9c81aa03d245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 3.09/7\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.62\n",
      "Length Appropriateness: 0.90\n",
      "Key Phrases: 0.04\n",
      "Sequence Alignment: 0.28\n",
      "Factual Accuracy: 0.36\n",
      "Coherence: 0.37\n",
      "\n",
      "Processing Time: 7.80 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Joel/Joel-D041-Q2.pdf\"\n",
    "teacher_answer = \"Q2-ii-answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8721615-3032-4a29-b235-d2d004a59215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.51/5\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.46\n",
      "Length Appropriateness: 0.63\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.18\n",
      "Factual Accuracy: 0.27\n",
      "Coherence: 0.17\n",
      "\n",
      "Processing Time: 6.72 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Joel/Joel-D041-Q3.pdf\"\n",
    "teacher_answer = \"Q3Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098ed841-cf38-42d3-9cdd-50986741539a",
   "metadata": {},
   "source": [
    "# Kalp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1293308a-890e-4f29-8561-a79a2d1dc3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 0.74/3\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.50\n",
      "Length Appropriateness: 0.19\n",
      "Key Phrases: 0.01\n",
      "Sequence Alignment: 0.16\n",
      "Factual Accuracy: 0.20\n",
      "Coherence: 0.21\n",
      "\n",
      "Processing Time: 6.34 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Kalp/Kalp-D043-Q1.pdf\"\n",
    "teacher_answer = \"Q1Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579789b8-a2fc-4cf1-a897-6ecfbb03e047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 3.87/7\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.72\n",
      "Length Appropriateness: 1.00\n",
      "Key Phrases: 0.09\n",
      "Sequence Alignment: 0.41\n",
      "Factual Accuracy: 0.82\n",
      "Coherence: 0.26\n",
      "\n",
      "Processing Time: 10.11 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Kalp/Kalp-D043-Q2.pdf\"\n",
    "teacher_answer = \"Q2-ii-answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9348fb3-4d0f-4dbe-855f-b626f7fa350c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.45/5\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.39\n",
      "Length Appropriateness: 0.51\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.21\n",
      "Factual Accuracy: 0.33\n",
      "Coherence: 0.30\n",
      "\n",
      "Processing Time: 6.51 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Kalp/Kalp-D043-Q3.pdf\"\n",
    "teacher_answer = \"Q3Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e4d6e-9f72-410c-9d03-258997fdbb57",
   "metadata": {},
   "source": [
    "# Krisha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf70739-1f6a-415a-9f72-abd7fe926cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.09/3\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.64\n",
      "Length Appropriateness: 0.36\n",
      "Key Phrases: 0.01\n",
      "Sequence Alignment: 0.20\n",
      "Factual Accuracy: 0.60\n",
      "Coherence: 0.27\n",
      "\n",
      "Processing Time: 7.56 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Krisha/Krisha-D053-Q1.pdf\"\n",
    "teacher_answer = \"Q1Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c4a81b0-d5b9-4843-909f-b005b48d8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 2.69/7\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.47\n",
      "Length Appropriateness: 0.74\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.13\n",
      "Factual Accuracy: 0.89\n",
      "Coherence: 0.24\n",
      "\n",
      "Processing Time: 7.41 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Krisha/Krisha-D053-Q2.pdf\"\n",
    "teacher_answer = \"Q2-i-answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29adbb4e-8dff-4fb2-a7d0-9bf62f4196fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.25/5\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.29\n",
      "Length Appropriateness: 0.49\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.19\n",
      "Factual Accuracy: 0.27\n",
      "Coherence: 0.36\n",
      "\n",
      "Processing Time: 5.56 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Krisha/Krisha-D053-Q3.pdf\"\n",
    "teacher_answer = \"Q3Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a69a4-90f0-416e-89a5-55a67ff907f6",
   "metadata": {},
   "source": [
    "# Mehika\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd2be1a-340c-42f9-829c-b9ddc9d5c553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.1/3\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.52\n",
      "Length Appropriateness: 0.49\n",
      "Key Phrases: 0.03\n",
      "Sequence Alignment: 0.24\n",
      "Factual Accuracy: 0.60\n",
      "Coherence: 0.37\n",
      "\n",
      "Processing Time: 7.00 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Mehika/Mehika-D062-Q1.pdf\"\n",
    "teacher_answer = \"Q1Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8745491-2204-46b6-9345-b63247dc2d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 3.14/7\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.55\n",
      "Length Appropriateness: 0.80\n",
      "Key Phrases: 0.08\n",
      "Sequence Alignment: 0.20\n",
      "Factual Accuracy: 0.73\n",
      "Coherence: 0.44\n",
      "\n",
      "Processing Time: 14.53 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Mehika/Mehika-D062-Q2.pdf\"\n",
    "teacher_answer = \"Q2-ii-answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09e1104-1948-47d6-9865-11dd2c328108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing student's submission...\n",
      "Converting PDF to images...\n",
      "Extracting text from images...\n",
      "Processing teacher's answer...\n",
      "Preprocessing texts...\n",
      "Evaluating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Grading Report ===\n",
      "Final Marks: 1.88/5\n",
      "\n",
      "Detailed Scores:\n",
      "Semantic Similarity: 0.42\n",
      "Length Appropriateness: 0.78\n",
      "Key Phrases: 0.00\n",
      "Sequence Alignment: 0.28\n",
      "Factual Accuracy: 0.40\n",
      "Coherence: 0.48\n",
      "\n",
      "Processing Time: 8.17 seconds\n"
     ]
    }
   ],
   "source": [
    "student_pdf = \"Mehika/Mehika-D062-Q3.pdf\"\n",
    "teacher_answer = \"Q3Answer.txt\"\n",
    "output_dir = \"output_images\"\n",
    "\n",
    "result = main(student_pdf, teacher_answer, output_dir, total_marks=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d72863-7923-41ad-859e-f2b1d1207320",
   "metadata": {},
   "source": [
    "# MULTIMODAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ab0eee-6009-4bec-a315-53f1f2e3e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 13:57:53.456130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743841673.574655    6062 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743841673.605107    6062 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 13:57:53.937321: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d43ba947bdf4c64b10d9fafcb904b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18291fce8b54300bb3ee033a43d8797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ea3399d1e441019bc0ba342b5e9f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1afe43cd11941f6821173f7e76de05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/802M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/microsoft/layoutlmv2-base-uncased/8cffd5ed065ff81e1e5c9a38968372c8541ecb8499999c89a8d9e10d65de3406?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1743844729&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mzg0NDcyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9taWNyb3NvZnQvbGF5b3V0bG12Mi1iYXNlLXVuY2FzZWQvOGNmZmQ1ZWQwNjVmZjgxZTFlNWM5YTM4OTY4MzcyYzg1NDFlY2I4NDk5OTk5Yzg5YThkOWUxMGQ2NWRlMzQwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Hl3tu1Xtjp6nZZl7EJNSdJF6Q5J9RmRXsk%7EnHHh6UymcZTImICqgBSWfpkQGf454L4AuH0weSLbq05Wt4tzqmQQ92VLbvV3s9jakfXA%7EVpNG-y4veZWkMMY-Rv3k9B%7EKGDbHylPMHp3o3x%7E4s0ZzLKekbN6qLiRFCG2PSV-fyaf17zeGaMU13yKSBisteVeMRGInMNqrvnYM9JxXCjBm5FZfFiGkYtRKPF0iHDznlu3dtllbL55%7EgXiMvQjUW0CL14fWMG4ChDVi8FQXz71XOwxh7nSZwhkrT61syYFno2zhB1mX9Swk8ZX%7Ewk24H0%7EMIBqXdn16TLuQKV-6dZjMdA__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'cdn-lfs.hf.co\\', port=443): Max retries exceeded with url: /microsoft/layoutlmv2-base-uncased/8cffd5ed065ff81e1e5c9a38968372c8541ecb8499999c89a8d9e10d65de3406?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1743844729&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mzg0NDcyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9taWNyb3NvZnQvbGF5b3V0bG12Mi1iYXNlLXVuY2FzZWQvOGNmZmQ1ZWQwNjVmZjgxZTFlNWM5YTM4OTY4MzcyYzg1NDFlY2I4NDk5OTk5Yzg5YThkOWUxMGQ2NWRlMzQwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Hl3tu1Xtjp6nZZl7EJNSdJF6Q5J9RmRXsk~nHHh6UymcZTImICqgBSWfpkQGf454L4AuH0weSLbq05Wt4tzqmQQ92VLbvV3s9jakfXA~VpNG-y4veZWkMMY-Rv3k9B~KGDbHylPMHp3o3x~4s0ZzLKekbN6qLiRFCG2PSV-fyaf17zeGaMU13yKSBisteVeMRGInMNqrvnYM9JxXCjBm5FZfFiGkYtRKPF0iHDznlu3dtllbL55~gXiMvQjUW0CL14fWMG4ChDVi8FQXz71XOwxh7nSZwhkrT61syYFno2zhB1mX9Swk8ZX~wk24H0~MIBqXdn16TLuQKV-6dZjMdA__&Key-Pair-Id=K3RPWS32NSSJCE (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e3310f3d700>: Failed to resolve \\'cdn-lfs.hf.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 023eb10e-36a8-4038-a842-e025222a7bf1)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:737\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 737\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/http/client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:861\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 861\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/response.py:742\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/models.py:826\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/util/connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7e3310f3d700>: Failed to resolve 'cdn-lfs.hf.co' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/adapters.py:589\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 589\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Max retries exceeded with url: /microsoft/layoutlmv2-base-uncased/8cffd5ed065ff81e1e5c9a38968372c8541ecb8499999c89a8d9e10d65de3406?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1743844729&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mzg0NDcyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9taWNyb3NvZnQvbGF5b3V0bG12Mi1iYXNlLXVuY2FzZWQvOGNmZmQ1ZWQwNjVmZjgxZTFlNWM5YTM4OTY4MzcyYzg1NDFlY2I4NDk5OTk5Yzg5YThkOWUxMGQ2NWRlMzQwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Hl3tu1Xtjp6nZZl7EJNSdJF6Q5J9RmRXsk~nHHh6UymcZTImICqgBSWfpkQGf454L4AuH0weSLbq05Wt4tzqmQQ92VLbvV3s9jakfXA~VpNG-y4veZWkMMY-Rv3k9B~KGDbHylPMHp3o3x~4s0ZzLKekbN6qLiRFCG2PSV-fyaf17zeGaMU13yKSBisteVeMRGInMNqrvnYM9JxXCjBm5FZfFiGkYtRKPF0iHDznlu3dtllbL55~gXiMvQjUW0CL14fWMG4ChDVi8FQXz71XOwxh7nSZwhkrT61syYFno2zhB1mX9Swk8ZX~wk24H0~MIBqXdn16TLuQKV-6dZjMdA__&Key-Pair-Id=K3RPWS32NSSJCE (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e3310f3d700>: Failed to resolve 'cdn-lfs.hf.co' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete. Results saved to results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 166\u001b[0m     ocr_system \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalOCRSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Process a document\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     result \u001b[38;5;241m=\u001b[39m ocr_system\u001b[38;5;241m.\u001b[39mprocess_document(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProperTesting/Jeet/Jeet-D038.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36mMultimodalOCRSystem.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Initialize OCR and LLM components\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m LayoutLMv2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/layoutlmv2-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLayoutLMv2ForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/layoutlmv2-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/modeling_utils.py:3627\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3624\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[0;32m-> 3627\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\n\u001b[1;32m   3629\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[1;32m   3631\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[1;32m   3632\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   3633\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3634\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[1;32m   3635\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[1;32m   3636\u001b[0m     )\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1230\u001b[0m     )\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:1381\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1379\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1381\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1392\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:1915\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m   1913\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1924\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1925\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:558\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    556\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    557\u001b[0m         reset_sessions()  \u001b[38;5;66;03m# In case of SSLError it's best to reset the shared requests.Session objects\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_resume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_nb_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_nb_retries\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    571\u001b[0m         consistency_error_message\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    572\u001b[0m             actual_size\u001b[38;5;241m=\u001b[39mtemp_file\u001b[38;5;241m.\u001b[39mtell(),\n\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    574\u001b[0m     )\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:455\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    453\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (resume_size,)\n\u001b[0;32m--> 455\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHF_HUB_DOWNLOAD_TIMEOUT\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m    459\u001b[0m content_length \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/file_download.py:387\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:93\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/requests/adapters.py:622\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'cdn-lfs.hf.co\\', port=443): Max retries exceeded with url: /microsoft/layoutlmv2-base-uncased/8cffd5ed065ff81e1e5c9a38968372c8541ecb8499999c89a8d9e10d65de3406?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1743844729&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mzg0NDcyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9taWNyb3NvZnQvbGF5b3V0bG12Mi1iYXNlLXVuY2FzZWQvOGNmZmQ1ZWQwNjVmZjgxZTFlNWM5YTM4OTY4MzcyYzg1NDFlY2I4NDk5OTk5Yzg5YThkOWUxMGQ2NWRlMzQwNj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=Hl3tu1Xtjp6nZZl7EJNSdJF6Q5J9RmRXsk~nHHh6UymcZTImICqgBSWfpkQGf454L4AuH0weSLbq05Wt4tzqmQQ92VLbvV3s9jakfXA~VpNG-y4veZWkMMY-Rv3k9B~KGDbHylPMHp3o3x~4s0ZzLKekbN6qLiRFCG2PSV-fyaf17zeGaMU13yKSBisteVeMRGInMNqrvnYM9JxXCjBm5FZfFiGkYtRKPF0iHDznlu3dtllbL55~gXiMvQjUW0CL14fWMG4ChDVi8FQXz71XOwxh7nSZwhkrT61syYFno2zhB1mX9Swk8ZX~wk24H0~MIBqXdn16TLuQKV-6dZjMdA__&Key-Pair-Id=K3RPWS32NSSJCE (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7e3310f3d700>: Failed to resolve \\'cdn-lfs.hf.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 023eb10e-36a8-4038-a842-e025222a7bf1)')"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "from transformers import LayoutLMv2Processor, LayoutLMv2ForSequenceClassification\n",
    "import torch\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "\n",
    "class MultimodalOCRSystem:\n",
    "    def __init__(self):\n",
    "        # Initialize OCR and LLM components\n",
    "        self.processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "        self.model = LayoutLMv2ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "        \n",
    "    def preprocess_image(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"Preprocess the input image for better OCR results\"\"\"\n",
    "        image = cv2.imread(image_path)\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # Apply threshold to get image with only black and white\n",
    "        _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "        return binary\n",
    "\n",
    "    def extract_text(self, image: np.ndarray) -> str:\n",
    "        \"\"\"Extract text from image using OCR\"\"\"\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text\n",
    "\n",
    "    def detect_question_numbers(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect question numbers from text\"\"\"\n",
    "        pattern = r'Q\\.?\\s*(\\d+)|Question\\s*(\\d+)'\n",
    "        matches = re.finditer(pattern, text)\n",
    "        question_numbers = [match.group(1) or match.group(2) for match in matches]\n",
    "        return question_numbers\n",
    "\n",
    "    def extract_answers(self, text: str, question_numbers: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Extract answers based on question numbers\"\"\"\n",
    "        answers = {}\n",
    "        text_lines = text.split('\\n')\n",
    "        current_question = None\n",
    "        current_answer = []\n",
    "\n",
    "        for line in text_lines:\n",
    "            # Check if line contains question number\n",
    "            for q_num in question_numbers:\n",
    "                if f\"Q{q_num}\" in line or f\"Question {q_num}\" in line:\n",
    "                    if current_question:\n",
    "                        answers[current_question] = ' '.join(current_answer)\n",
    "                    current_question = q_num\n",
    "                    current_answer = []\n",
    "                    break\n",
    "            else:\n",
    "                if current_question:\n",
    "                    current_answer.append(line)\n",
    "\n",
    "        # Add the last answer\n",
    "        if current_question:\n",
    "            answers[current_question] = ' '.join(current_answer)\n",
    "\n",
    "        return answers\n",
    "\n",
    "    def extract_numericals(self, text: str, question_numbers: List[str]) -> Dict[str, List[float]]:\n",
    "        \"\"\"Extract numerical values based on question numbers\"\"\"\n",
    "        numericals = {}\n",
    "        for q_num in question_numbers:\n",
    "            # Find the section of text corresponding to this question\n",
    "            pattern = f\"Q{q_num}.*?(?=Q{int(q_num)+1}|$)\"\n",
    "            question_text = re.search(pattern, text, re.DOTALL)\n",
    "            if question_text:\n",
    "                # Extract all numbers from the question text\n",
    "                numbers = re.findall(r'-?\\d*\\.?\\d+', question_text.group())\n",
    "                numericals[q_num] = [float(num) for num in numbers]\n",
    "        return numericals\n",
    "\n",
    "    def detect_diagrams(self, image: np.ndarray, question_numbers: List[str]) -> Dict[str, Dict]:\n",
    "        \"\"\"Detect and classify DSA diagrams\"\"\"\n",
    "        diagrams = {}\n",
    "        \n",
    "        # Convert image to binary\n",
    "        _, binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for q_num in question_numbers:\n",
    "            diagrams[q_num] = {\n",
    "                'type': None,\n",
    "                'components': []\n",
    "            }\n",
    "            \n",
    "            # Analyze contours to identify diagram types and components\n",
    "            for contour in contours:\n",
    "                # Get bounding box\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                \n",
    "                # Basic shape analysis for diagram classification\n",
    "                aspect_ratio = float(w)/h\n",
    "                area = cv2.contourArea(contour)\n",
    "                \n",
    "                # Simple classification logic (can be enhanced with ML)\n",
    "                if aspect_ratio > 0.8 and aspect_ratio < 1.2:\n",
    "                    # Might be a node\n",
    "                    diagrams[q_num]['components'].append({\n",
    "                        'type': 'node',\n",
    "                        'position': (x, y)\n",
    "                    })\n",
    "                elif aspect_ratio > 2:\n",
    "                    # Might be an edge/link\n",
    "                    diagrams[q_num]['components'].append({\n",
    "                        'type': 'edge',\n",
    "                        'position': (x, y)\n",
    "                    })\n",
    "                \n",
    "            # Determine diagram type based on components\n",
    "            if len(diagrams[q_num]['components']) > 0:\n",
    "                diagrams[q_num]['type'] = self.classify_diagram_type(diagrams[q_num]['components'])\n",
    "                \n",
    "        return diagrams\n",
    "\n",
    "    def classify_diagram_type(self, components: List[Dict]) -> str:\n",
    "        \"\"\"Classify diagram type based on components\"\"\"\n",
    "        nodes = sum(1 for c in components if c['type'] == 'node')\n",
    "        edges = sum(1 for c in components if c['type'] == 'edge')\n",
    "        \n",
    "        if nodes == 0:\n",
    "            return None\n",
    "        elif edges == 0:\n",
    "            return 'single_node'\n",
    "        elif edges == nodes - 1:\n",
    "            return 'tree'\n",
    "        elif edges >= nodes:\n",
    "            return 'graph'\n",
    "        else:\n",
    "            return 'linked_list'\n",
    "\n",
    "    def process_document(self, image_path: str) -> Dict:\n",
    "        \"\"\"Process document and extract all required information\"\"\"\n",
    "        # Preprocess image\n",
    "        processed_image = self.preprocess_image(image_path)\n",
    "        \n",
    "        # Extract text\n",
    "        text = self.extract_text(processed_image)\n",
    "        \n",
    "        # Detect question numbers\n",
    "        question_numbers = self.detect_question_numbers(text)\n",
    "        \n",
    "        # Extract answers\n",
    "        answers = self.extract_answers(text, question_numbers)\n",
    "        \n",
    "        # Extract numericals\n",
    "        numericals = self.extract_numericals(text, question_numbers)\n",
    "        \n",
    "        # Detect diagrams\n",
    "        diagrams = self.detect_diagrams(processed_image, question_numbers)\n",
    "        \n",
    "        return {\n",
    "            'answers': answers,\n",
    "            'numericals': numericals,\n",
    "            'diagrams': diagrams\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    ocr_system = MultimodalOCRSystem()\n",
    "    \n",
    "    # Process a document\n",
    "    result = ocr_system.process_document('ProperTesting/Jeet/Jeet-D038.pdf')\n",
    "    \n",
    "    # Save results to JSON\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(result, f, indent=4)\n",
    "    \n",
    "    print(\"Processing complete. Results saved to results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c97525-5a28-4e92-a62f-71fc45581141",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e019831-ee4a-4c5d-88b3-943457091ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 19:05:36.077215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744810536.090076   23984 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744810536.093921   23984 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-16 19:05:36.108046: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/dhruv/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "[nltk_data] Downloading package punkt to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dhruv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "from pdf2image import convert_from_path\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "\n",
    "# Download required NLP resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3da233-9eda-433b-969b-ba9c730d9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=\"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\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32148339-d68f-45d0-9116-2fd55dbb508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_images(pdf_path, output_folder=\"images\"):\n",
    "    \"\"\"Convert PDF pages to image files.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    images = convert_from_path(pdf_path)\n",
    "    image_paths = []\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        path = os.path.join(output_folder, f\"page_{i+1}.jpg\")\n",
    "        image.save(path, \"JPEG\")\n",
    "        image_paths.append(path)\n",
    "\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dac6bfb1-922f-480f-ba3a-a9260a65588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_images(image_paths):\n",
    "    \"\"\"Use Kimi VL model to extract only the visible text from each image, with no extra content.\"\"\"\n",
    "    all_text = \"\"\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        with open(image_path, \"rb\") as img_file:\n",
    "            base64_image = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"moonshotai/kimi-vl-a3b-thinking:free\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": (\n",
    "                                \"Transcribe all the visible text from this image exactly as it is. \"\n",
    "                                \"Do not explain, summarize, or interpret anything. \"\n",
    "                                \"Do not include any thought process or internal reasoning like ◁think▷. \"\n",
    "                                \"Only return the raw visible text, exactly as it appears in the image.\"\n",
    "                            )\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        all_text += completion.choices[0].message.content + \"\\n\"\n",
    "\n",
    "    return all_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98483f49-07a0-424e-9276-f71a2b8dbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_to_text(pdf_path):\n",
    "    print(\"[1/2] Converting PDF to images...\")\n",
    "    image_paths = pdf_to_images(pdf_path)\n",
    "\n",
    "    print(\"[2/2] Extracting text using Kimi VL...\")\n",
    "    extracted_text = extract_text_from_images(image_paths)\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f2fc72-6ad0-45bf-b766-cc28c5279f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] Converting PDF to images...\n",
      "[2/2] Extracting text using Kimi VL...\n",
      "◁think▷I need to transcribe all the visible text from the provided image exactly as it is. I should not include any explanations, summaries, interpretations, or thought processes. I should only return the raw visible text.\n",
      "\n",
      "The image appears to be a handwritten document with several points about asymptotic notation and algorithm complexities. Let me transcribe each visible line carefully:\n",
      "\n",
      "1. The first line seems to be \"01\" with some markings next to it.\n",
      "2. The \"Ans\" label followed by numbered points:\n",
      "   - Point 1: \"Asymptotic Defined that a function is tending to one axis you can say like it is a limit Example of Asymptotic Function could be e²\"\n",
      "   - Point 2: \"Asymptotic notation in Data Structure are Used to Represent time Complexities of your Algorithm\"\n",
      "   - Point 3: \"It tells about the worst, average and Best Case time Complexity i.e time taken to execute a particular algorithm or Code\"\n",
      "   - Point 4: \"The denotions or Best,Worst and average case are : O(Big O) → worst case Ω (Omega) → Best case Θ (theta) → Average case\"\n",
      "\n",
      "I need to make sure my transcription is accurate and captures all the handwritten text exactly as it appears. I should not correct any spelling or formatting, as the instruction is to transcribe it exactly as it is.\n",
      "\n",
      "Let me now prepare the transcription:◁/think▷# Transcription of Visible Text:\n",
      "\n",
      "```\n",
      "Q.1\n",
      "Ans\n",
      "1) Asymptotic Defined that a function is tending to one axis you can say like it is a limit Example of Asymptotic Function could be e²\n",
      "2) Asymptotic notation in Data Structure are Used to Represent time Complexities of your Algorithm\n",
      "3) It tells about the worst, average and Best Case time Complexity i.e time taken to execute a particular algorithm or Code\n",
      "4) The denotions or Best, Worst and average case are : O(Big O) → worst case Ω (Omega) → Best case Θ (theta) → Average case\n",
      "```\n",
      "◁ dhe◁\n",
      "\n",
      "Question Nos. Marks Awarded\n",
      "3\n",
      "5) But In data structure We\n",
      "Always (consider that what Is\n",
      "the cost Case of An Algorithm\n",
      "or Code that define that how\n",
      "much Computational power Computer\n",
      "to Execute.\n",
      "6) Ex :\n",
      "int count = O\n",
      "for (int i = 0 ; i < n ; i++)\n",
      "{\n",
      "count ++;\n",
      "}\n",
      "In this Code the loop has\n",
      "time complexity of O (n) and\n",
      "the count variable updation\n",
      "has O (1)\n",
      "Time Complexity = O(n)\n",
      "7) O(1) > O (log n) < O (Jin) < O (n)\n",
      "< O(n²) < O (n³) ... < O(2n) < O (n!)\n",
      "This Is an increasing order (growth\n",
      "func)\n",
      "0\n",
      "0(n)\n",
      "O(n)\n",
      "O(√n)\n",
      "O(log n)\n",
      "O(n)◁/think▷**Transcription:**\n",
      "\n",
      "```\n",
      "3\n",
      "\n",
      "5) But In data structure We Always (consider that what Is the cost Case of An Algorithm or Code that define that how much Computational power Computer to Execute.\n",
      "\n",
      "6) Ex :\n",
      "int count = O\n",
      "for (int i = 0 ; i < n ; i++)\n",
      "{\n",
      "count ++;\n",
      "}\n",
      "\n",
      "In this Code the loop has time complexity of O (n) and the count variable updation has O (1) Time Complexity = O(n)\n",
      "\n",
      "7) O(1) > O (log n) < O (Jin) < O (n) < O(n²) < O (n³) ... < O(2n) < O (n!) This Is an increasing order (growth func)\n",
      "\n",
      "This is an increasing order (growth func)\n",
      "0\n",
      "O(n)\n",
      "O(n)\n",
      "O(√n)\n",
      "O(log n)\n",
      "O(n)\n",
      "``````\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = \"Jeet/Jeet-D038-Q1.pdf\"  # <- Replace this\n",
    "extracted_text = process_pdf_to_text(pdf_file_path)\n",
    "\n",
    "# You now have your result in `extracted_text`\n",
    "print(extracted_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e932b2d-a5fc-40f1-872f-76406677b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_thoughts(text):\n",
    "    return re.sub(r\"◁think▷.*?◁/think▷\", \"\", text, flags=re.DOTALL).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3a0527-f38b-4c98-bc18-285f3a1b77b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Transcription of Visible Text:\\n\\n```\\nQ.1\\nAns\\n1) Asymptotic Defined that a function is tending to one axis you can say like it is a limit Example of Asymptotic Function could be e²\\n2) Asymptotic notation in Data Structure are Used to Represent time Complexities of your Algorithm\\n3) It tells about the worst, average and Best Case time Complexity i.e time taken to execute a particular algorithm or Code\\n4) The denotions or Best, Worst and average case are : O(Big O) → worst case Ω (Omega) → Best case Θ (theta) → Average case\\n```\\n◁ dhe◁\\n\\nQuestion Nos. Marks Awarded\\n3\\n5) But In data structure We\\nAlways (consider that what Is\\nthe cost Case of An Algorithm\\nor Code that define that how\\nmuch Computational power Computer\\nto Execute.\\n6) Ex :\\nint count = O\\nfor (int i = 0 ; i < n ; i++)\\n{\\ncount ++;\\n}\\nIn this Code the loop has\\ntime complexity of O (n) and\\nthe count variable updation\\nhas O (1)\\nTime Complexity = O(n)\\n7) O(1) > O (log n) < O (Jin) < O (n)\\n< O(n²) < O (n³) ... < O(2n) < O (n!)\\nThis Is an increasing order (growth\\nfunc)\\n0\\n0(n)\\nO(n)\\nO(√n)\\nO(log n)\\nO(n)◁/think▷**Transcription:**\\n\\n```\\n3\\n\\n5) But In data structure We Always (consider that what Is the cost Case of An Algorithm or Code that define that how much Computational power Computer to Execute.\\n\\n6) Ex :\\nint count = O\\nfor (int i = 0 ; i < n ; i++)\\n{\\ncount ++;\\n}\\n\\nIn this Code the loop has time complexity of O (n) and the count variable updation has O (1) Time Complexity = O(n)\\n\\n7) O(1) > O (log n) < O (Jin) < O (n) < O(n²) < O (n³) ... < O(2n) < O (n!) This Is an increasing order (growth func)\\n\\nThis is an increasing order (growth func)\\n0\\nO(n)\\nO(n)\\nO(√n)\\nO(log n)\\nO(n)\\n``````'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_output = strip_thoughts(extracted_text)\n",
    "clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b34bf0b-4f2d-4e59-a25c-72a99bf46b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08eea7d6-dfbb-4c07-88fe-442061264365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name().replace(\"_\", \" \"))  \n",
    "    return synonyms\n",
    "\n",
    "def get_semantic_factual_similarity(student_text, reference_text):\n",
    "    embeddings1 = sbert_model.encode(student_text, convert_to_tensor=True)\n",
    "    embeddings2 = sbert_model.encode(reference_text, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(embeddings1, embeddings2).item()\n",
    "\n",
    "    def extract_numbers(text):\n",
    "        return set(re.findall(r'\\d+(?:\\.\\d+)?', text))\n",
    "\n",
    "    student_numbers = extract_numbers(student_text)\n",
    "    reference_numbers = extract_numbers(reference_text)\n",
    "    factual_accuracy = len(student_numbers.intersection(reference_numbers)) / len(reference_numbers) if reference_numbers else 1.0\n",
    "\n",
    "    return (similarity + factual_accuracy) / 2  \n",
    "\n",
    "def check_length_ratio(student_text, reference_text):\n",
    "    student_length = len(word_tokenize(student_text))\n",
    "    reference_length = len(word_tokenize(reference_text))\n",
    "    ratio = student_length / reference_length if reference_length > 0 else 0\n",
    "    return min(1.0, ratio if ratio <= 1.5 else 1.5 / ratio)\n",
    "\n",
    "def check_sequence_alignment(student_text, reference_text):\n",
    "    student_sentences = sent_tokenize(student_text)\n",
    "    reference_sentences = sent_tokenize(reference_text)\n",
    "    \n",
    "    sent_ratio = min(len(student_sentences), len(reference_sentences)) / max(len(student_sentences), len(reference_sentences))\n",
    "    \n",
    "    student_paragraphs = student_text.split('\\n\\n')\n",
    "    reference_paragraphs = reference_text.split('\\n\\n')\n",
    "    para_ratio = min(len(student_paragraphs), len(reference_paragraphs)) / max(len(student_paragraphs), len(reference_paragraphs))\n",
    "    \n",
    "    return (sent_ratio + para_ratio) / 2\n",
    "\n",
    "def check_key_phrases(student_text, reference_text):\n",
    "    def get_phrases(text):\n",
    "        words = word_tokenize(text.lower())\n",
    "        phrases = set()\n",
    "        for i in range(len(words)-1):\n",
    "            phrases.add(f\"{words[i]} {words[i+1]}\")\n",
    "            if i < len(words)-2:\n",
    "                phrases.add(f\"{words[i]} {words[i+1]} {words[i+2]}\")\n",
    "        return phrases\n",
    "    \n",
    "    ref_phrases = get_phrases(reference_text)\n",
    "    student_phrases = get_phrases(student_text)\n",
    "    \n",
    "    expanded_ref_phrases = set()\n",
    "    for phrase in ref_phrases:\n",
    "        words = phrase.split()\n",
    "        for word in words:\n",
    "            expanded_ref_phrases.update(get_synonyms(word))\n",
    "        expanded_ref_phrases.add(phrase)\n",
    "\n",
    "    student_embedding = sbert_model.encode(student_text, convert_to_tensor=True)\n",
    "    ref_embedding = sbert_model.encode(list(expanded_ref_phrases), convert_to_tensor=True)\n",
    "    scores = util.pytorch_cos_sim(student_embedding, ref_embedding).tolist()[0]\n",
    "    \n",
    "    phrase_match_score = len(student_phrases.intersection(expanded_ref_phrases)) / len(expanded_ref_phrases) if expanded_ref_phrases else 0\n",
    "    bert_match_score = max(scores) if scores else 0\n",
    "\n",
    "    return max(phrase_match_score, bert_match_score)\n",
    "\n",
    "def check_coherence(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < 2:\n",
    "        return 1.0\n",
    "            \n",
    "    coherence_scores = []\n",
    "    for i in range(len(sentences)-1):\n",
    "        emb1 = sbert_model.encode(sentences[i])\n",
    "        emb2 = sbert_model.encode(sentences[i+1])\n",
    "        similarity = util.pytorch_cos_sim(emb1, emb2)\n",
    "        coherence_scores.append(similarity.item())\n",
    "            \n",
    "    return sum(coherence_scores) / len(coherence_scores) \n",
    "\n",
    "# --- Master Evaluation Function ---\n",
    "\n",
    "def evaluate_student_answer(student_text: str, reference_path: str, total_marks: float = 10.0):\n",
    "    with open(reference_path, 'r', encoding='utf-8') as f:\n",
    "        reference_text = f.read()\n",
    "\n",
    "    scores = {\n",
    "        'semantic_factual_similarity': get_semantic_factual_similarity(student_text, reference_text),\n",
    "        'length_appropriateness': check_length_ratio(student_text, reference_text),\n",
    "        'sequence_alignment': check_sequence_alignment(student_text, reference_text),\n",
    "        'key_phrases': check_key_phrases(student_text, reference_text),\n",
    "        'coherence': check_coherence(student_text)\n",
    "    }\n",
    "\n",
    "    weights = {\n",
    "        'semantic_factual_similarity': 0.68,\n",
    "        'length_appropriateness': 0.22,\n",
    "        'sequence_alignment': 0.02,\n",
    "        'key_phrases': 0.04,\n",
    "        'coherence': 0.04\n",
    "    }\n",
    "\n",
    "    weighted_score = sum(scores[metric] * weights[metric] for metric in scores)\n",
    "    scaled_score = weighted_score * total_marks  # Scale to total marks\n",
    "\n",
    "    return {\n",
    "        'final_score': round(scaled_score, 2),\n",
    "        'details': {k: round(v, 3) for k, v in scores.items()}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f2d0e3-e8d0-4cd5-b4f0-60afbc1c77a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Scaled Score: 1.47\n",
      "Component Breakdown: {'semantic_factual_similarity': 0.431, 'length_appropriateness': 0.73, 'sequence_alignment': 0.273, 'key_phrases': 0.493, 'coherence': 0.264}\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_student_answer(extracted_text, reference_path='Q1Answer.txt', total_marks=3)\n",
    "\n",
    "print(\"Final Scaled Score:\", result['final_score'])\n",
    "print(\"Component Breakdown:\", result['details'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
